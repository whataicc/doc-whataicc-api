[{"id":0,"href":"/docs/otherai/chat/chatbox/","title":"ChatBox(推荐使用)","section":"对话客户端类","content":" Chatbox教程(免费)快速安装与使用全攻略，一键连接所有AI语言模型 # 一、ChatBox 介绍与功能 # Chatbox AI是一款集多模型对话、文件解析、AI绘图、代码生成等功能于一体的全能AI助手，支持Windows、Mac、Linux、iOS、Android 及网页全平台。凭借简洁直观的界面设计、强大的本地化处理能力和开箱即用的云端服务，Chatbox AI 已成为提升工作效率的首选工具。\n核心功能与特色 # 1. 智能对话系统\n多模型对比：支持同时连接 ChatGPT 5/Claude 4/DeepSeek 等先进模型，自由切换体验不同AI风格 上下文记忆：自动保存完整对话历史，支持 Markdown/HTML/TXT 多种格式导出 个性化设置：可自定义温度、最大token等参数，精准控制AI输出 2. 生产力增强套件\n文件理解：直接解析PDF/Word/Excel/图片中的内容 代码专家：提供类 Claude Artifacts 的实时代码预览能力 实时联网：一键获取最新资讯、股票行情等网络数据，回答始终与时俱进 3. 创意可视化工具\nAI绘图：通过自然语言描述生成高质量图像（每月免费额度） 图表生成：支持将数据转化为可视化图表，支持 LaTeX 公式渲染 多格式渲染：完美呈现 Markdown、HTML、数学公式等专业内容 4. 企业级数据管理\n本地优先：所有对话记录和文件默认存储在用户设备，保障数据主权 技术优势 # 零门槛体验：无需技术背景，安装即可享受 AI 生产力 持续进化：同步更新模型库和功能模块，保持技术前沿性 适用场景 # 学术研究：快速解析论文/教材，生成文献综述 内容创作：辅助写作、翻译、社交媒体文案生成 开发编程：代码生成/调试/文档编写全流程支持 商业分析：自动处理报表数据，生成可视化洞察 二、Chatbox主要功能 # 与文档和图片聊天 # 无论是文档、图片还是代码，只需将您的文件发送给 Chatbox。它将理解内容并提供智能响应，在每次交互中提高您的生产力和创造力。\n代码神器：生成与预览 # 编码更高效，不再费力。AI 助手让创意化为现实 - 更智能，更迅速。\n实时联网搜索与查询 # 通过 AI 联网搜索获取即时的信息。保持最新事实、新闻和来自互联网的数据。\nAI 生成的图表可视化见解 # 无缝集成到对话中，这些可视化效果清晰地阐明复杂概念、趋势和统计数据。在需要时使用清晰、可定制的视觉辅助工具增强您的理解力和决策力。\nAI 驱动的图像生成 # 使用 Chatbox 的图像生成功能将您的想法变为现实。只需描述您的愿景，并观看我们的 AI 将您的文字转化为令人惊叹的视觉效果。\n三、Chatbox安装与配置 # 1. 下载 ChatBox # 前往 ChatBox GitHub 下载最新版本。 根据操作系统选择对应安装包： Windows → .exe macOS → .dmg Linux → .AppImage 2. 配置 API # 首次启动时：\n打开 ChatBox 设置（Settings）。 在 API Key 栏填写你在 神马中转API 平台 获取的 Key，例如： sk-xxxxxxx 在 API Base URL 填写： https://api.whatai.cc 保存即可。 👉 从此，无论你调用 OpenAI、Claude 还是 DeepSeek，都不需要单独配置 Key，全部走 神马中转API（api.whatai.cc）。\n使用方法：如图在设置中填入API密钥和API主机，并将API主机设置为https://api.whatai.cc即可\n不同的客户端需要填写不同的BASE_URL, 请尝试如下地址 https://api.whatai.cc https://api.whatai.cc/v1 https://api.whatai.cc/v1/chat/completions\nClaude配置：\n四、ChatBox 怎么用 # 打开 ChatBox → 点击 新建对话。 在模型选择栏里，选择你要使用的模型（ChatGPT / Claude / DeepSeek）。 输入问题，按 回车 即可与 AI 交互。 历史会话会自动保存，可以随时切换和管理。 五、ChatBox 怎么选择模型 # 在新建对话时，选择 神马中转API 提供的模型列表：\nChatGPT (OpenAI 系列) # gpt-5 gpt-4o gpt-4 gpt-3.5 Claude (Anthropic 系列) # claude-opus-4-1 claude-3-opus claude-3-sonnet claude-3-haiku DeepSeek # deepseek-chat（通用对话） deepseek-coder（编程优化） 参数设置 # temperature：控制创意程度（0~1）。 max_tokens：最大输出长度。 六、示例教程 # 下面给出二个主流模型的使用示例（全部走 神马中转API）。\n1. ChatGPT (OpenAI) # 配置：\nAPI Base URL → https://api.whatai.cc API Key → 【神马中转API Key】 模型 → gpt-5-mini 示例：\n帮我写一个Python脚本，读取CSV并计算平均值 👉 ChatGPT 会返回完整的 Python 示例代码。\n2. Claude (Anthropic) # 配置：\nAPI Base URL → https://api.whatai.cc/v1 API Key → 【神马中转API Key】 模型 → claude-opus-4-1-20250805 示例：\n请写一个JavaScript函数，计算数组的中位数 👉 Claude 会生成带注释的 完整代码。\n七、总结 # ChatBox 是一款非常实用的桌面 AI 客户端\n•\t不需要频繁切换网页 •\t支持多家大模型 •\t界面简洁，功能强大 •\t非常适合开发者、写作者、研究人员使用 ✅ 完成以上配置后，你就能高效使用 ChatBox！\n"},{"id":1,"href":"/docs/otherai/devtools/claudecode/","title":"Claude Code安装与国内使用教程","section":"编程工具类","content":" 「2025最新推荐」Claude Code国内使用_保姆级新手安装使用教程_神马中转API Claude Code代理API_最强AI编程工具 # 什么是 Claude Code # Claude Code 是 Anthropic 推出的一个 agentic 编码工具 (agentic coding tool)，可以在命令行（terminal）中运行，或者集成在一些支持终端的 IDE 中，借助 Claude 的语言模型能力来辅助写代码、重构、调试、维护、理解代码库等。\nClaude Code特点 # •\t能理解整个代码库的上下文，不只是单个文件； •\t支持自然语言命令 —— 用 “说”的方式让它做事情，比如 “帮我重构这个函数”、“让这个模块更高效”、“在这个地方加测试” 等； •\t可以执行命令／运行 shell 或 bash 命令并把输出作为上下文之一； •\t支持项目记忆（persistent project context），比如通过 CLAUDE.md 文件提供项目的风格、结构、常用脚本等，这样 Claude 在后续操作里就能“记得”这些规则。 🚀 主要功能 # 智能代码生成 - 快速生成高质量代码\n代码分析 - 深度理解和分析代码结构\n调试助手 - 智能发现和修复代码问题\n文档生成 - 自动生成代码文档\n命令行集成 - 无缝集成到开发流程\n⭐神马中转API专属功能 # 神马中转API（api.whatai.cc）所有LLM 模型均支持在 Claude code 中使用\n如果 Claude code 无法修改调用模型，可参考教程令牌中，设置模型转发\n📦 安装步骤 # Mac \u0026amp; Liunx 配置方式 # 1. 安装 Node.js # 确保系统已安装 Node.js 18+ 版本\n安装 Homebrew (mac推荐) # 如果尚未安装 Homebrew：\n/bin/bash -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\u0026#34; 2. 安装 Node.js # 使用 Homebrew：\nbrew install node 2. 安装 Claude Code # npm install -g @anthropic-ai/claude-code 3. 配置 API 密钥 # 获取 Auth Token (参考添加令牌文档 # 方法一：使用 Bash（推荐） # echo \u0026#39;export ANTHROPIC_AUTH_TOKEN=\u0026#34;sk-xxx\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.bash_profile echo \u0026#39;export ANTHROPIC_BASE_URL=\u0026#34;https://api.whatai.cc\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.bash_profile source ~/.bash_profile 方法二：使用 Zsh（如果使用 Oh My Zsh） # echo \u0026#39;export ANTHROPIC_AUTH_TOKEN=\u0026#34;sk-xxx\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.zshrc echo \u0026#39;export ANTHROPIC_BASE_URL=\u0026#34;https://api.whatai.cc\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.zshrc source ~/.zshrc 注意： 永久设置后需要重启终端才能生效。\n4. 启动使用 Claude Code # # 进入项目目录 cd your-project-folder # 启动 Claude Code claude 首次启动后需要先进行主题的选择等操作：\n• 选择喜欢的主题（回车）\n• 确认安全须知（回车）\n• 使用默认 Terminal 配置（回车）\n• 信任工作目录（回车）\n• 开始编程！🚀\nWindows配置方式 # 1、管理员权限启动cmd\n1. 安装 Node.js # 访问 Node.JS 官网，点击最新版本，选择对应的操作系统和版本下载即可\n下载后双击安装，之后一直点击下一步。安装完成后，打开 CMD 窗口，执行命令验证安装：\nnode -v 2、设置 npm 配置,告诉 npm 在安装包时忽略执行包中的脚本（如 preinstall、postinstall 等）,设置完重新启动CMD\nsetx NPM_CONFIG_IGNORE_SCRIPTS true 3、安装 Claude Code\nnpm install -g @anthropic-ai/claude-code 4、配置 SHELL 环境变量\n方法一：图形化配置（推荐，永久生效） # a. 右键点击 \u0026ldquo;此电脑\u0026rdquo; → 选择 \u0026ldquo;属性\u0026rdquo;\nb. 点击 \u0026ldquo;高级系统设置\u0026rdquo;\nc. 在 \u0026ldquo;系统属性\u0026rdquo; 窗口中点击 \u0026ldquo;环境变量\u0026rdquo;\n重要：在 \u0026ldquo;系统变量\u0026rdquo; 部分点击 \u0026ldquo;新建\u0026rdquo;（多人共享电脑可选择 \u0026ldquo;用户变量\u0026rdquo;）\nd. 添加以下两个变量：\n变量名：ANTHROPIC_AUTH_TOKEN，变量值：sk-xxx\n变量名：ANTHROPIC_BASE_URL，变量值：https://api.whatai.cc\ne. 点击 \u0026ldquo;确定\u0026rdquo; 保存\n方法二：PowerShell（永久设置） # [Environment]::SetEnvironmentVariable(\u0026#34;ANTHROPIC_AUTH_TOKEN\u0026#34;, \u0026#34;sk-xxx\u0026#34;, \u0026#34;User\u0026#34;) [Environment]::SetEnvironmentVariable(\u0026#34;ANTHROPIC_BASE_URL\u0026#34;, \u0026#34;https://api.whatai.cc\u0026#34;, \u0026#34;User\u0026#34;) 方法三：命令提示符（永久设置） # CMD\nsetx ANTHROPIC_AUTH_TOKEN \u0026#34;sk-xxx\u0026#34; setx ANTHROPIC_BASE_URL \u0026#34;https://api.whatai.cc\u0026#34; 注意： 永久设置后需要重启终端才能生效。推荐使用永久配置方式。\n方法四：通过settings.json 设置 # 找到 settings.json 文件，如果没有请创建\nC:\\Users\\{user}\\.claude\\settings.json 设置 API 信息，保存\n{ \u0026#34;env\u0026#34;: { \u0026#34;ANTHROPIC_MODEL\u0026#34;: \u0026#34;claude-sonnet-4-20250514\u0026#34;, \u0026#34;ANTHROPIC_SMALL_FAST_MODEL\u0026#34;: \u0026#34;claude-sonnet-4-20250514\u0026#34;, \u0026#34;ANTHROPIC_BASE_URL\u0026#34;: \u0026#34;https://api.whatai.cc\u0026#34;, \u0026#34;ANTHROPIC_AUTH_TOKEN\u0026#34;: \u0026#34;sk-AG2\u0026#34; } } setx SHELL \u0026#34;C:\\Program Files\\Git\\bin\\bash.exe\u0026#34; #这里要换成你的路径， # 如果不知道，可以执行 where git 找一下 5、添加 npm 环境变量\nC:\\Users\\y.xie\\.npm-global # 添加Windows环境变量，同样要设置为你的路径，在npm安装包里面 # 如果不知道，可以执行 npm config get prefix 找一下 将其添加到 Windows 的环境变量（PATH），关闭并重新打开你的终端窗口（CMD / PowerShell / Git Bash），使设置生效。\n6、设置API配置\n7、重启开发环境\n8、enjoy!!\n# 进入项目目录 cd your-project-folder # 启动 Claude Code claude 🎯 常用命令 # claude - 启动交互模式\nclaude \u0026quot;task\u0026quot; - 运行一次性任务\nclaude commit - 创建 Git 提交\n/help - 显示可用命令\n/clear - 清除对话历史\n/review - 请求代码审查\n💡 使用示例 # # 代码生成 \u0026gt; 请帮我写一个 Python 函数，用于计算斐波那契数列 # 代码审查 claude \u0026#34;review this code for potential bugs\u0026#34; # 自动提交 claude commit 切换模型 # 使用 Claude Code 命令：\n/model [model id] 默认模型为Sonnet 4，你可以用效果更好的Opus 4：\nopus\n/model opus 或者，换成其他 claude 模型：\nsonnet 3.7\nsonnet 3.5\n/model claude-3-7-sonnet-20250219 Kimi K2 支持启动 Claude Code 之后，只需要运行指令\n/model moonshotai/kimi-k2-instruct 其他 LLM 模型均支持使用，比如 Openai、Gemini、Qwen、Doubao\n⚠️ 重要提示 # API 密钥配置：请将 sk-your-api-key 替换为您在 本站 生成的实际 API 密钥\n令牌分组：在 本站 创建令牌时，建议选择 \u0026ldquo;企业分组 官转分组\u0026rdquo;\n网络连接：确保网络连接稳定，工具需要与 API 服务器通信\n项目目录：建议在具体项目目录下使用，以获得更好的上下文理解\n🔧 高级功能 # IDE 集成 - 支持 Cursor 等 IDE 集成\nMCP 服务器 - 扩展 Agent 能力\nCI/CD 集成 - 自动化代码审查流程\n团队规范 - 通过CLAUDE.md文件定义团队规范\nClaude Code 能做什么：功能列表 # 下面是 Claude Code 在实际开发中能帮你做的事情，列出来比较全，也分几类：\n类别 功能 代码生成 新建模块/组件/API/数据库模型 重构优化 改变量名、优化性能、清理废代码 调试修复 定位 bug、写单元测试、解决依赖冲突 文档注释 自动生成 docstring / README / 使用指南 跨语言转换 Python ↔ JS / Go / Java 等 代码审查 检查安全性、性能、风格一致性 脚本任务 执行测试、构建、生成脚手架、CI/CD 配置 理解项目 帮助快速熟悉陌生/遗留代码库 Claude Code 实用教程：高效开发技巧与最佳实践 # 一、快速项目初始化与脚手架生成 # 1.1 创建全栈应用脚手架 # 使用场景 # 需要快速搭建一个包含前后端的完整项目结构，包括配置文件、Docker支持、测试框架等。\n操作步骤 # # 在终端中打开目标目录，然后启动 Claude claude Prompt 示例 # 请帮我创建一个全栈应用的项目结构，要求： 1. 后端：Node.js + Express + TypeScript + Prisma ORM 2. 前端：React + TypeScript + Vite + TailwindCSS 3. 包含 Docker Compose 配置（前端、后端、PostgreSQL、Redis） 4. 包含 ESLint、Prettier 配置 5. 包含 GitHub Actions CI/CD 配置 6. 包含完整的 README.md 项目名称：task-manager 请生成完整的目录结构和所有必要的配置文件。 预期输出结构 # task-manager/ ├── backend/ │ ├── src/ │ │ ├── controllers/ │ │ ├── middlewares/ │ │ ├── models/ │ │ ├── routes/ │ │ ├── services/ │ │ ├── utils/ │ │ └── index.ts │ ├── prisma/ │ │ └── schema.prisma │ ├── tests/ │ ├── .env.example │ ├── Dockerfile │ ├── package.json │ └── tsconfig.json ├── frontend/ │ ├── src/ │ │ ├── components/ │ │ ├── hooks/ │ │ ├── pages/ │ │ ├── services/ │ │ ├── utils/ │ │ ├── App.tsx │ │ └── main.tsx │ ├── Dockerfile │ ├── package.json │ ├── vite.config.ts │ └── tsconfig.json ├── docker-compose.yml ├── .github/ │ └── workflows/ │ └── ci.yml ├── .gitignore └── README.md 1.2 生成 API 端点与文档 # Prompt 示例 # 基于以下数据模型，生成完整的 RESTful API： 模型： - User (id, email, name, role, createdAt, updatedAt) - Task (id, title, description, status, priority, assigneeId, createdAt, updatedAt) - Comment (id, taskId, userId, content, createdAt) 要求： 1. 为每个模型生成完整的 CRUD 端点 2. 包含身份验证中间件（JWT） 3. 包含输入验证（使用 Joi 或 Zod） 4. 包含分页、排序、过滤功能 5. 生成 OpenAPI/Swagger 文档 6. 包含示例请求和响应 输出格式：完整的 routes 文件和 controller 文件 二、智能代码重构与优化 # 2.1 性能优化分析 # 使用场景 # 发现代码性能问题，需要 Claude 分析并提供优化方案。\nPrompt 示例 # 分析以下 React 组件的性能问题并提供优化方案： [粘贴你的代码] 请从以下角度分析： 1. 不必要的重渲染 2. 内存泄漏风险 3. 大列表渲染优化 4. 异步操作优化 5. Bundle size 优化 输出： - 问题清单（按严重程度排序） - 每个问题的具体优化方案 - 优化后的完整代码 - 性能提升预期 2.2 代码现代化升级 # Prompt 示例 # 将以下 JavaScript 代码升级为现代 TypeScript，要求： 1. 添加完整的类型定义 2. 使用 ES6+ 特性（async/await、解构、模板字符串等） 3. 改进错误处理 4. 添加 JSDoc 注释 5. 遵循 TypeScript 最佳实践 [粘贴旧代码] 额外要求： - 保持向后兼容 - 列出所有破坏性变更 - 提供迁移指南 三、测试代码生成与覆盖率提升 # 3.1 生成完整测试套件 # Prompt 示例 # 为以下模块生成完整的测试套件： [粘贴你的代码模块] 测试要求： 1. 单元测试（Jest/Vitest） 2. 集成测试 3. 边界条件测试 4. 错误场景测试 5. Mock 外部依赖 6. 测试覆盖率目标：\u0026gt;90% 输出： - 完整的测试文件 - 测试用例说明表格 - Mock 数据生成函数 - 测试运行配置 3.2 E2E 测试脚本生成 # Prompt 示例 # 基于以下用户流程生成 Playwright E2E 测试： 用户流程： 1. 用户访问登录页 2. 输入邮箱和密码 3. 点击登录 4. 跳转到仪表板 5. 创建新任务 6. 编辑任务状态 7. 删除任务 8. 登出 要求： - 使用 Page Object Model - 包含断言验证 - 处理异步操作 - 添加截图功能 - 支持多浏览器测试 四、数据库操作与迁移 # 4.1 数据库 Schema 设计与优化 # Prompt 示例 # 设计一个电商系统的数据库 schema，要求： 业务需求： - 用户管理（多角色） - 商品管理（多分类、多规格） - 订单系统（状态流转） - 库存管理（实时更新） - 支付记录 - 评价系统 技术要求： 1. 使用 PostgreSQL 2. 考虑索引优化 3. 考虑数据分区 4. 包含触发器和存储过程 5. 生成 Prisma schema 6. 生成 SQL 迁移脚本 7. 包含种子数据脚本 输出格式： - ER 图描述 - SQL DDL 语句 - Prisma schema - 索引优化建议 4.2 数据迁移脚本生成 # Prompt 示例 # 生成数据迁移脚本，将 MongoDB 数据迁移到 PostgreSQL： MongoDB 集合结构： { users: { _id, email, profile: { name, avatar }, posts: [postIds] }, posts: { _id, userId, title, content, tags: [], comments: [{}] } } 目标 PostgreSQL 结构： - users 表 - profiles 表（1对1） - posts 表 - tags 表 - post_tags 关联表 - comments 表 要求： 1. 处理数据类型转换 2. 处理关联关系 3. 包含回滚脚本 4. 批量处理大数据量 5. 错误处理和日志 6. 进度显示 五、API 集成与调试 # 5.1 生成 API 客户端 # Prompt 示例 # 基于以下 OpenAPI 规范生成 TypeScript API 客户端： [粘贴 OpenAPI JSON/YAML] 要求： 1. 完整的类型定义 2. 支持请求拦截器 3. 自动重试机制 4. 错误处理封装 5. 支持取消请求 6. 缓存机制 7. 生成使用示例 技术栈：Axios + TypeScript 5.2 Mock Server 生成 # Prompt 示例 # 基于以下 API 规范创建 Mock Server： API 列表： - GET /api/users（分页） - GET /api/users/:id - POST /api/users - PUT /api/users/:id - DELETE /api/users/:id - GET /api/users/:id/posts 要求： 1. 使用 Express + Faker.js 2. 支持动态数据生成 3. 模拟延迟和错误 4. 支持 WebSocket 5. 数据持久化（JSON 文件） 6. 支持场景切换 7. 包含 Docker 配置 六、调试与问题定位 # 6.1 错误分析与修复 # Prompt 示例 # 分析以下错误并提供解决方案： 错误信息： [粘贴完整的错误堆栈] 相关代码： [粘贴相关代码片段] 环境信息： - Node.js version: 18.x - Framework: Next.js 14 - Database: PostgreSQL 15 要求： 1. 解释错误原因 2. 提供多种解决方案 3. 推荐最佳方案 4. 提供修复后的代码 5. 如何避免类似问题 6.2 性能问题诊断 # Prompt 示例 # 诊断以下性能问题： 症状： - API 响应时间从 100ms 增加到 2s - 数据库查询缓慢 - 内存使用持续增长 提供的信息： [慢查询日志] [相关代码] [性能监控数据] 请提供： 1. 问题根因分析 2. 优化方案（短期/长期） 3. 具体代码修改 4. 性能监控建议 5. 预防措施 七、文档生成与维护 # 7.1 生成项目文档 # Prompt 示例 # 为项目生成完整文档： 项目结构： [粘贴项目树形结构] 要求生成： 1. README.md（项目介绍、快速开始、架构说明） 2. API 文档（端点说明、请求示例、响应格式） 3. 部署文档（环境要求、部署步骤、配置说明） 4. 开发指南（代码规范、提交规范、分支策略） 5. 故障排查指南 格式要求： - Markdown 格式 - 包含目录 - 包含示例代码 - 包含架构图（Mermaid） 7.2 代码注释生成 # Prompt 示例 # 为以下代码添加详细注释： [粘贴代码] 注释要求： 1. JSDoc/TSDoc 格式 2. 函数说明（目的、参数、返回值、异常） 3. 复杂逻辑说明 4. 算法时间/空间复杂度 5. 使用示例 6. 注意事项 7. TODO 和 FIXME 标记 八、CI/CD 配置生成 # 8.1 GitHub Actions 工作流 # Prompt 示例 # 生成 GitHub Actions CI/CD 配置： 项目信息： - 前端：React + TypeScript - 后端：Node.js + TypeScript - 数据库：PostgreSQL - 部署目标：AWS ECS 要求： 1. PR 检查（lint、test、build） 2. 主分支自动部署到 staging 3. Tag 触发生产部署 4. 包含密钥管理 5. 并行执行优化 6. 缓存优化 7. 通知机制（Slack） 生成： - .github/workflows/ci.yml - .github/workflows/deploy.yml - 部署脚本 九、代码审查与安全扫描 # 9.1 安全漏洞扫描 # Prompt 示例 # 对以下代码进行安全审查： [粘贴代码] 检查项： 1. SQL 注入 2. XSS 攻击 3. CSRF 攻击 4. 敏感信息泄露 5. 不安全的依赖 6. 权限验证漏洞 7. 加密问题 8. 输入验证 输出： - 漏洞列表（按严重等级） - 修复方案 - 修复后的代码 - 安全最佳实践建议 "},{"id":2,"href":"/docs/otherai/sdk/openaisdk/","title":"OpenAI官方SDK","section":"官方SDK类","content":" OpenAI官方SDK ​ # 使用 OpenAI API 查询 GPT-4 # 以下是一个示例代码，演示如何使用 OpenAI API 查询 GPT-4 模型的回答。\n示例代码 ​ # import openai def query_gpt4(question): openai.api_key = \u0026#34;sk-xxx\u0026#34; # 替换为您的 API 密钥 # openai.base_url = url openai.base_url = \u0026#39;https://api.whatai.cc/v1/\u0026#39; # 设置 API 基础 URL try: response = openai.chat.completions.create( model=\u0026#34;gpt-4\u0026#34;, # 确认使用 GPT-4 模型 messages=[ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;You are a helpful assistant.\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: question} ] ) print(response) # 打印完整的响应 return response[\u0026#39;choices\u0026#39;][0].message[\u0026#39;content\u0026#39;] # 返回 GPT-4 的回答 except Exception as e: return str(e) # 返回错误信息 # 问题 question = \u0026#34;为什么太阳那么红？\u0026#34; # 获取并打印回答 answer = query_gpt4(question) print(answer) 注意事项 ​ # 确保您替换, sk-xxx, 为您的有效 API 密钥。\n使用正确的 API 基础 URL，并根据需要修改其他设置。\n"},{"id":3,"href":"/docs/otherai/tools/raycast/","title":"Raycast插件ChatGPT使用指南（推荐使用）","section":"工具类","content":" Raycast 插件 ChatGPT 使用指南（推荐使用） # Raycast 是一款高效的快捷启动与工作流管理工具，可以替代 Spotlight，更快地打开应用、执行脚本、调用 API、管理工作效率插件。它不仅适合开发者，也适合日常办公、学习场景。\n一、Raycast基础功能与特点 # 一、Raycast 基础功能\n应用启动\n按 ⌘ + 空格 或自定义快捷键呼出 Raycast\n输入应用名称即可快速打开\n文件与文件夹搜索\n输入文件名、文件夹名即可快速定位\n支持预览、复制路径\n系统控制\n快速调整音量、亮度\n切换 Wi-Fi、蓝牙等\n工作流与插件（Extensions）\nRaycast 内置商店（Store）\n可以安装各类生产力工具（如 GitHub、Jira、Notion、ChatGPT 等插件）\n二、Raycast 的特点\n快速启动：几乎零延迟的搜索和应用启动体验。\n可扩展性强：内置插件市场，开发者可以编写并分享自己的插件。\n快捷命令：支持自定义脚本（Shell、Python、JavaScript），快速执行。\n工作流整合：支持与 GitHub、Notion、Jira、Linear 等工具无缝结合。\n美观 \u0026amp; 极简：界面清爽、交互直观，键盘操作流畅无比。\n免费为主：核心功能免费，Pro 版提供团队协作和更强的个性化功能。\n二、安装与配置 Raycast # 下载安装\n官网：https://www.raycast.com\n支持 macOS\n初始设置\n设置全局快捷键（推荐：⌘ + 空格 替换 Spotlight）\n登录账号（方便同步插件和配置）\n安装插件\n打开 Raycast → 输入 Store → 搜索需要的插件 → 一键安装 三、插件推荐（高频实用🔥） # 以下推荐一些实用的 Raycast 插件，分为通用类和开发类：\n🔧 通用效率类 # ChatGPT\n直接在 Raycast 中调用 GPT 模型，快速生成文本、写代码、翻译。 Clipboard History\n剪贴板管理神器，快速查找复制过的内容。 Emoji Search\n通过关键字快速搜索 Emoji。 Google Translate\n内置翻译功能，支持中英文互译。 Timer \u0026amp; Stopwatch\n设置倒计时、专注时间。 💻 开发者类 # GitHub\n查看 PR、Issue、仓库，快速跳转到网页。 Jira\n管理任务、查看工单，适合团队协作。 Notion\n快速搜索和打开页面。 Linear\n管理任务，和敏捷开发结合良好。 Docker\n查看容器状态，直接管理容器。 🌐 网络与工具类 # Google Search\n快速搜索 Google 结果。 Currency Converter\n实时汇率转换。 Weather\n查看当前天气。 Raindrop.io\n管理书签收藏。 四、Raycast 插件 ChatGPT 使用指南（重点 🔥） # Raycast 的 ChatGPT 插件可以让你在任何场景下快速调用 GPT 能力，不需要额外切换到浏览器。\n1. 安装 ChatGPT 插件 # 打开 Raycast → 输入 Store\n搜索 ChatGPT 插件\n点击 Install 安装\n在 Raycast Store 中找到 ChatGPT 插件，并按照提示安装：\n2. API Key 配置 # ChatGPT 插件依赖 OpenAI API或者使用【神马中转API】，你需要：\n前往 OpenAI API Keys或者国内直连使用【神马中转API（api.whatai.cc）】\n复制 API Key\n打开 Raycast → 输入 ChatGPT → 设置 → 粘贴 API Key\n安装完成后在该插件配置中的 API Key 中填入【神马中转API】的API Key，以及选中 Change API Endpoint，并在 API Endpoint 中填入 https://api.whatai.cc/v1\n3. 快捷键设置 # 你可以为 ChatGPT 插件 设置全局快捷键，比如：⌘ + ⇧ + G\n随时唤出 ChatGPT，输入问题即可得到回答\n4. 使用场景 # 日常问答：数学题、语法解释、编程调试\n文本处理：中英文翻译、润色、写邮件\n开发辅助：代码解释、生成正则、写脚本\n🍺 enjoy it~ 五、ChatGPT 插件使用示例 # 以下是一些常见的使用场景示例：\n示例 1：写代码 # 用户输入： 用 Python 写一个快速排序 ChatGPT 输出： def quick_sort(arr): if len(arr) \u0026lt;= 1: return arr pivot = arr[len(arr)//2] left = [x for x in arr if x \u0026lt; pivot] middle = [x for x in arr if x == pivot] right = [x for x in arr if x \u0026gt; pivot] return quick_sort(left) + middle + quick_sort(right) 示例 2：文本润色 # 用户输入： 请润色这句话 → “我很喜欢用raycast，因为它帮我提高效率” ChatGPT 输出： “我非常喜欢使用 Raycast，它极大地提升了我的工作效率。” 示例 3：快速学习 # 用户输入： 请用简单的方式解释二叉树 ChatGPT 输出： 二叉树是一种数据结构，每个节点最多有两个子节点：左子树和右子树。就像一个家谱，每个人最多有两个孩子。 六、总结 # Raycast 是一个效率神器，结合插件可以替代大量工具\nChatGPT 插件让 AI 随时随地可用，提升学习、写作、编程效率\n通过配置快捷键，你几乎可以把 ChatGPT 当成 操作系统级别的助手\n"},{"id":4,"href":"/docs/aidocs/base/","title":"基本概念-令牌（Token）","section":"基础知识","content":" 📖基本概念-令牌（Token） # 1️⃣ 基本概念 # 别称：密钥、key、token\n用于身份验证核心凭证，关联用户账户信息等\n用户可自主管理令牌，支持以下操作\n✅ 新增多个令牌\n✅ 设置单令牌额度上限\n✅ 指定调用渠道分组（如ssvip、default）\n✅ 绑定调用模型权限\n📖 示例格式（非真实token）：\nsk-UEE3xe6AgDeAvlCUQsP0hJSdyaOsNpByoMtb99CC8POogslU\n💻 管理入口：\n👉 神马中转API令牌控制台\n❓ 用户高频疑问解密 # Q：如何获取 Openai Key？ 如何获取 Claude Key？ 如何获取 gemini key？\n→ A：Key就是令牌！这个key可以使用站内支持模型页面介绍的所有模型（260+），GPT系列模型、Claude系列模型、gemini系列模型等等等。\nQ：key为什么不能用？为什么我调用API没反应？\n→ A：大部分情况是 Base url设置不正确造成。需要把Openai的Baseurl改成 https://api.whatai.cc\nQ: 单个key的并发或RPM、TPM有限制吗？\n→ A：: 没有限制。神马中转API不会主动限制用户 RPM TPM ，但所有模型账号都是所有用户共享，遇到使用高峰可能会 429 或 500 报错。高并发需求用户可以寻找客服咨询。\n"},{"id":5,"href":"/docs/ailearn/wallet/","title":"基础教程-钱包 · 页面","section":"站内基础教程","content":" 基础教程-钱包 · 页面 # 💰 我的钱包：您的AI算力加油站 # 余额即能量！ 钱包有余额才能激活令牌（Key），开启AI超能力——就像手机要先充值才能通话📱\n💻 充值入口：\n👉神马中转API钱包\n🎁 新人大礼包 # 注册即送$0.2算力额度 ≈ 免费体验10次深度问答 或 200次短对话！\n🎉 双通道充值系统 # 方式 操作步骤 到账速度 客服支持 兑换码充能 1. 联系客服获取算力额度 2. 钱包页输入兑换码 3. 点击「兑换」按钮 5分钟 ✅ 专属客服对接 闪电充值 1. 进入钱包页 2. 点击「支付宝/微信/USDT」图标 3. 扫码支付完成 秒到账 自动到账 🔋 最低充能：$1起充（≈50次短问答）\n🌟 邀请秘诀 # 邀请3位好友各充30算力额度等于您白赚9算力（足够生成3份商业计划书！）\n"},{"id":6,"href":"/docs/introduction/","title":"神马中转API简介","section":"神马中转API 一站式AI大模型API聚合平台 · 行业领先","content":" 神马中转API一站式AI大模型API中转站 · 低价好用稳定的中转API服务 # 神马聚合中转API是一个高效的Open AI、Midjourney API代理、Claude代理、Suno代理等供应商 我们致力于提供优质的 API 接入服务，让您可以轻松集成先进的AI模型至您的产品和服务。通过 API 综合管理平台，无缝整合OpenAl最尖端的人工智能模型。借助我们可靠且易于使用的API解决方案，升级您的产品与服务。\n神马中转API支持全球主流顶尖AI大模型生态，包括OpenAI的GPT-5/GPT-4o/GPT-4.1系列、谷歌Gemini 2.5 Pro/Flash/nano-banana、Anthropic Claude Sonnet 4.5/4.1，以及 DeepSeek-v3、Midjourney、Luma、Llama3、Mistral等开源模型，覆盖文本、图像、语音、视频等多模态场景，满足从高精度推理到高效生成的多样化需求。\n随着大模型与人工智能应用的普及，越来越多的开发者与企业希望快速集成各类AI接口。然而，直接调用不同厂商的API往往面临接口差异大、计费复杂、访问不稳定等问题。这时候，神马中转API中转站的价值就凸显出来了。\n什么是AI API中转站？ # AI API中转站，顾名思义，就是一个聚合、统一和转发AI接口调用的服务平台。开发者无需分别对接多个厂商的API，只需通过中转站，就能调用不同模型与服务。它相当于“中间层”，帮助开发者屏蔽底层的复杂性，提供更加稳定、高效、灵活的接入方式。\nAI中转站的核心优势 # 1. 统一接口标准 # • 各家大模型（如OpenAI、Anthropic、Google、智谱、月之暗面等）的API参数各不相同，直接调用容易增加学习和维护成本。\n• 中转站将不同接口进行标准化，开发者只需学习一套规范，即可调用多个模型。\n2. 提升访问稳定性 # • 直连官方API可能会受到网络环境、节点延迟或不可用情况影响。\n• 中转站往往部署了全球加速和负载均衡，确保请求更稳定、更顺畅。\n3. 聚合计费与灵活套餐 # • 不同厂商的API价格策略不一，充值也要分别管理。\n• 中转站支持统一计费、统一充值，并且可以提供更灵活的套餐选择，降低使用门槛。\n4. 支持多模型切换 # • 一个项目可能需要不同模型（例如GPT适合文本生成，Claude适合长文档处理，本土模型更适合中文任务）。\n• 中转站可以让你在一条API内快速切换和调用不同模型，极大提升开发效率。\n5. 额外功能与增强服务 # • 一些中转站提供调用日志、使用统计、速率限制优化、故障自动切换等附加功能。\n• 对企业级用户，还可能支持私有化部署与团队协作。 为什么选择【神马聚合中转API】（api.whatai.cc）？ # 在众多中转服务中，神马聚合中转API（api.whatai.cc）凭借稳定性、易用性与性价比脱颖而出：\n• 多模型支持：支持国内外主流大模型，满足不同场景的需求。\n• 高可用性：全球加速+负载均衡，保证请求稳定性与低延迟。\n• 统一调用规范：开发者只需学习一次，即可调用所有支持的AI接口。\n• 灵活计费：按量计费、套餐灵活，避免资金分散在多个平台。\n• 开发者友好：提供完善的文档、调用示例与调试工具，接入成本低。\n适用场景 # • 个人开发者：想要快速体验不同模型，无需注册多个平台。\n• 企业应用：需要在不同业务中调用多个模型，降低运维与成本压力。\n• 创业团队：快速验证产品原型，避免API接入的重复工作。\n主流模型全支持 # 聚合中国和全球300+多模态大模型 文生文、文生图、文生视频、文生音频 神马中转API汇聚了国内外300+多模态大型人工智能模型，全面支持文生文、文生图、文生视频、文生音频等多种模态API。这些模型基于深度学习技术，具备强大的自然语言处理、图像识别、视频分析和音频合成能力，让您的AI应用如虎添翼。核心优势包括海量模型资源、多样化生成能力、智能优化算法和简单易用等。应用场景广泛，包括营销推广、内容创作、教育培训和娱乐互动等。\nOpenAI 顶级大模型 # 全球顶级人工智能AI大模型\nGPT-5\nModel List: GPT-5 gpt-5 GPT-5 gpt-5-mini GPT-4.1 gpt-4.1 GPT-4.1 gpt-4.1-mini o1 \u0026amp; o3 o1 / o3-mini GPT-4 gpt-4o GPT-4 gpt-4o-mini Text-to-Image gpt-image-1 Text-to-Speech whisper-1 \u0026hellip; 谷歌 AI大模型 # OpenAI的主要竞争对手，Gemini系列以强大的推理、多模态能力著称\ngemini-2.5-pro、nano-banana、gemini-2.5-flash-lite等\nModel List: gemini-2.5-pro gemini-2.5-flash-lite nano-banana veo2-pro gemini-2.5-flash-thinking gemini-2.0-flash gemini-1.5-pro-latest gemini-2.5-flash-image \u0026hellip; Anthropic AI大模型 # 专注安全与对齐，Claude 系列对话体验和长文本处理能力极强\nClaude Sonnet 4.5\nModel List: Claude Sonnet 4.5 claude-4.1 claude-opus-4-1-20250805 claude-4 claude-sonnet-4-20250514 claude-4 claude-opus-4-20250514 claude-3.7 claude-3-7-sonnet claude-3.5 claude-3-5-sonnet claude-3 claude-3-opus-20240229 claude-3 claude-3-haiku-20240307 claude-3 claude-3-haiku \u0026hellip; 开源大模型 # 主流开源人工智能大模型，灵活可部署，社区生态活跃\nDeepSeek\nModel List: DeepSeek R1 deepseek-reasoner DeepSeek V3 deepseek-chat Llama3.3 llama-3.3-8b-instant Llama3 llama-3-70b code-llama code-llama-34b code-llama code-llama-13b mistral mistral-large-latest mistral mistral-medium-latest luma kling-video-v1 jimeng-videos runwayml-gen4_turbo \u0026hellip; 如何快速接入神马聚合中转API # ⚡3步闪电接入【神马聚合中转API】（api.whatai.cc） · 智启全球260+顶尖AI · 自由选源 精准控成本\n✅极简通用流程：\n开发者｜企业｜创作者 — 仅需3步，零配置调用 ChatGPT、Claude、Gemini 等 260+ 全球模型！\n支持多源渠道接入 · 价格透明对比 · 按预算自由选择\n✅专属核心价值：\n开发者｜低成本集成 + 灵活选型降本，高效构建AIGC 应用\n企业｜API驱动自动化流程 + 预算可控，智能服务升级\n创作者｜零基础玩转AI 创作全场景 + 丰俭由人选渠道\n让复杂归简 · 让创新加速！\n立即体验：【神马聚合中转API】（api.whatai.cc） 🏁 我们的优势 # 🌟 我们100%采用企业高速渠道，无套路无广告无保留聊天数据\n🌟 性价比最高的稳定三无纯净 API 源头\n🌟 覆盖全球8大地区，包括美国、日本、韩国、英国、新加坡、香港、菲律宾和俄罗斯，共计服务10万+满意客户\n🌟 稳定运行18个月，我们承诺永久优质服务\n♥ 选择我们，就是选择高效与可靠 # ❤ 无需科学上网，全球直连，无封号风险，请求速度是个人账号的1200倍\n❤ 无需模型权限，直接使用最新模型，无需开发基础，一个API key全模型通用\n❤ 完全兼容OpenAI接口协议，支持无缝对接所有模型到各种支持接口的应用\n❤ API key可设定使用时间和余额，便于二次销售 ❤ 100％保障隐私，仅做API中转\n❤ 享受我们的渠道优势，价格远低于官方\n❤ 支持超多模型、各种渠道，价格 \u0026amp; 质量都有保证\n总结 # AI API中转站的出现，大大简化了AI接口的接入与管理成本。通过 神马聚合中转API（api.whatai.cc），开发者和企业能够以更低成本、更高效率、更稳定的方式调用全球主流AI模型，为智能应用的落地加速。\n"},{"id":7,"href":"/docs/openai/syfw/","title":"适用范围：♥ 所有模型","section":"Openai请求格式（通用）","content":" 适用范围：♥ 所有模型 # API 统一请求格式 # 所有模型（包括非OpenAI模型）的请求格式已统一为OpenAI格式。\n功能特点 # 使用OpenAI请求格式可调用所有模型（Claude、Gemini、Doubao、Qwen等）\n支持直接使用OpenAI SDK调用任意模型\n基础信息 # Base URL: https://api.whatai.cc\n认证方式: API Key (替换为******)\nPython示例代码 # import openai # 配置API密钥 openai.api_key = \u0026#34;******\u0026#34; # 替换为你的实际API密钥 openai.api_base = \u0026#34;https://api.whatai.cc\u0026#34; # 设置API基础地址 # 调用示例（以Claude模型为例） response = openai.ChatCompletion.create( model=\u0026#34;claude\u0026#34;, # 指定模型名称 messages=[ {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;你好\u0026#34;} ] ) print(response.choices[0].message.content) 注意：实际使用时请将******替换为你的真实API密钥\n"},{"id":8,"href":"/docs/guide/","title":"快速使用指南","section":"神马中转API 一站式AI大模型API聚合平台 · 行业领先","content":" 🖥️ 1、注册「神马中转API」账号 # 免费注册：注册将自动赠送 0.2 美金余额，测试阶段跑通绝对没问题~\n立即注册\n🔑 2、获得 API Key # 进入后台的 令牌 页面，选择右上角【添加令牌】，填写名称其他默认，即可获取生成的key\nKey 的格式通常是：sk-xxxx......\n🔄 3、修改代码的请求地址 # 调用方式 # 只需要将请求地址和 Key 修改为与 OpenAI 官方完全一致。\n简单来说：将 https://api.openai.com 全局替换改成： https://api.whatai.cc\n并使用在『神马中转API』网站后台的 令牌，其他代码层面无需做任何改动。\n模型方面 # 本站支持大部分OpenAI、Claude、Gemini、Nano Banana等可用模型，注册即可试用。\n账户菜单 \u0026gt; 模型价格 \u0026gt; 一键复制模型名称\n注意事项 # 不同的客户端需要填写不同的BASE_URL，某些程序需要添加 v1 后缀，具体有3种情况，请测试：\nhttps://api.whatai.cc https://api.whatai.cc/v1 https://api.whatai.cc/v1/chat/completions 神马中转API可视化操作演示 # 顶部菜单选择聊天，里面有可视化操作【聊天\u0026amp;画图\u0026amp;视频\u0026amp;音乐】【Nano Banner画图】【GPT画图】\nNano Banner画图 # 默认配置，选择文生图或者图生图，输入提示词、选择数量/质量/比例，点击【生成】，等待生成完成第一时间下载图片不要刷新页面\n页面上的余额张数都不用管，是错误的，可以正常使用\n聊天\u0026amp;画图\u0026amp;视频\u0026amp;音乐 # 默认配置，选择模型，是输入问题，发送等待回复\n操练场-终极测试所有模型 # 操练场，正常默认配置，选择模型，输入问题，发送\n每次请求前点击输入框旁边的垃圾桶，清除内容之后在使用 如果需要传图片可以随便找个免费图床即可，推荐聚合图床 "},{"id":9,"href":"/docs/otherai/chat/chatgptwebmidjourneyproxy/","title":"Chatgpt-web-midjourney-proxy","section":"对话客户端类","content":" Chatgpt-web-midjourney-proxy配置教程 # ChatGPT \u0026amp; Midjourney配置教程： # 神马中转API聊天菜单 - ChatGPT \u0026amp; Midjourney - 左下角设置按钮 - 服务端 - 找到Midjourney设置：\n神马中转API聊天菜单 - ChatGPT \u0026amp; Midjourney - 绘画：\n提示词\nOn the streets of the city after the rain, the second-dimensional beauty wears a silver-gray hip skirt and gray stockings with patterns. Her high heels are eye-catching, in gray and deep purple tones. She has an urban fashion style. The stockings have exquisite patterns. The street lights and rainwater reflect the elegant legs. "},{"id":10,"href":"/docs/otherai/sdk/geminisdk/","title":"Gemini SDK配置","section":"官方SDK类","content":" 🌟 Gemini Python SDK 入门文档 # 支持 Gemini 官方格式； 所有模型均支持使用 Gemini 格式\n🔧 1. 安装与设置 # 获取 API 密钥 # 在网站 https://api.whatai.cc/token 获取令牌\n🚀 2. 生成简单文本 # CUSTOM_BASE_URL // 网站 BaseURL 通常是域名 API_KEY // 令牌页获取的令牌 非流式\nfrom google import genai from google.genai import types client = genai.Client( http_options=types.HttpOptions( base_url= CUSTOM_BASE_URL ), api_key= API_KEY ) response = client.models.generate_content( model=\u0026#34;gemini-2.5-flash\u0026#34;, contents=\u0026#34;How does AI work?\u0026#34; ) print(response.text) "},{"id":11,"href":"/docs/otherai/devtools/pythonopenai/","title":"python openai官方库（使用AutoGPT，langchain等）","section":"编程工具类","content":" python openai官方库（使用AutoGPT，langchain等） # 方法一\nimport openai openai.api_base = \u0026#34;https://api.whatai.cc/v1\u0026#34; 方法二（方法一不起作用用这个）\n修改环境变量OPENAI_API_BASE，各个系统怎么改环境变量请自行搜索，修改环境变量后不起作用请重启系统。\nOPENAI_API_BASE=https://api.whatai.cc/v1 "},{"id":12,"href":"/docs/otherai/devtools/cline/","title":"VSCode插件Cline使用指南","section":"编程工具类","content":" VSCode插件Cline使用+配置 # Cline 是一款新型的 VSCode AI 助手插件，主打 正确性 + 可控性 + 文件级操作能力。它让 AI 可以在你的本地项目中 读写文件、执行命令、运行代码、生成多步计划，更像一个可以合作的软件工程师。\n本文将带你从零开始学习 Cline，包括：\nCline 的核心功能和使用方式\n基础工作流程\n常见的几个使用示例\n如何通过神马中转API配置Cline使用GPT-5.1与Claude\n什么是Cline？它能做什么？ # Cline 是一个 VSCode 插件，让你可以在编辑器中与强大的 LLM（如 GPT-5.1、Claude 4.5 等）协作。它不仅仅是聊天，而是真正可以在项目中执行操作的助手。\n核心能力：\n✅文件操作（读/写/修改）\nCline 能浏览项目中的文件，并以高可控性方式修改。\n✅多步骤推理与执行计划\nCline 会先生成一份清晰的「操作计划」，你同意后才执行，每一步你都能检查。\n✅命令执行（终端）\nCline 能运行命令，比如 npm、git、python、docker 等。\n✅环境访问\n可读的代码文件、终端输出、目录结构。\n✅可使用多种模型\n如 GPT-5.1、Claude 3.5 Sonnet 等。\n如何安装配置Cline # 在 VSCode 扩展市场搜索：\nCline 发布者一般是：Roo Code / Cline.bot\n点击 Install 安装即可。\n打开后，你会在 VSCode 左下角看到一个 Cline 入口图标。\nCline配置模型：使用「神马中转 API」接入GPT-5.1/Claude # 这是很多人最关心的部分。下面教你完整配置方法。\n进入 Cline 设置\nVSCode → 右下角 Cline 图标 → 点击 Settings（设置）****\n主要关注三项：\nAPI Provider\nBase URL\nAPI Key\n选择 API Provider 为 “OpenAI Compatible” 设置 API Base URL（神马中转API地址）\n你的 Base URL 一般类似：\nhttps://api.whatai.cc/v1 填写 API Key\nAPI Key: sk-xxxxxxxxxxxxx 设置模型 在 Cline 的模型选择中填入你想用的模型名称，例如：\ngpt-5-2025-08-07 验证是否连接成功\n在 Cline 输入：\n你好，测试一下连接是否成功。 如果模型能正常响应，说明：\n✔️ API Key 正常\n✔️ Base URL 正常\n✔️ 模型名称有效\n配置Claude步骤类似，注意一下Base URL 不同的客户端需要填写不同的BASE_URL, 请尝试如下地址： https://api.whatai.cc https://api.whatai.cc/v1 https://api.whatai.cc/v1/chat/completions 你现在即可使用 GPT-5.1 / Claude 驱动 Cline 在 VSCode 中全自动工作。\nCline的基础使用流程 # Cline 的交互主要通过 右下角的任务面板 进行。\n典型流程：\n① 提出你的任务\n你可以告诉 Cline：\n“帮我重写这个函数的结构”\n“给我生成一个 Python 可视化示例”\n“把这个项目转成 TypeScript”\n“生成一个基于 Next.js 的登录界面”\n② Cline 会生成「行动计划」\nCline 会解析你的请求，并提出一份计划：\nPlan: 1. 读取 src/pages/index.js 2. 修改其中的内容 3. 新建 components/LoginForm.tsx 4. 运行 `npm install bcryptjs` ③ 你逐步确认\n你可以：\n✔️ 同意执行\n❌ 拒绝某一步\n✏️ 修改计划\n④ Cline 逐步执行操作\n包括：\n修改文件内容\n打开/创建文件\n执行终端命令\n返回结果并进入下一步\nCline的常见使用场景示例 # 下面给你几个非常实用的示例案例。\n示例 1：重构一个文件 # 你告诉 Cline：\n请重构当前文件，让代码更简洁清晰，同时保持功能不变。 Cline 的典型计划：\n查看文件内容\n输出重构后的代码\n替换文件\n你确认后它会自动完成重构。\n示例 2：生成一个新的React组件 # 你输入：\n帮我创建一个响应式的登录界面组件，使用 Tailwind，带 email + password 输入框。 Cline 会自动：\n创建 Login.tsx\n加入 Tailwind 类\n若缺依赖会提醒安装\n询问是否写测试\n你逐步确认，它即可生成完整文件结构。\n示例 3：执行终端命令 + 生成文件 # 你输入：\n创建一个 Python 项目，并生成一个 CSV 转 JSON 的脚本。 Cline 计划：\n新建目录 python-tools\n创建 convert.py\n添加 CSV → JSON 转换代码\n运行 python convert.py sample.csv\nCline 执行时，你每步都能看到终端输出。\n示例 4：自动修复一个报错 # 你输入：\n帮我修复这个 TypeScript 报错。 Cline 会：\n读取报错\n打开相应源文件\n分析类型问题\n提供修复方案\n推荐你确认后自动修改\n# 现在你已经掌握：\nCline 的核心使用方式\n多步骤计划与文件/命令操作\n如何使用 Cline 完成多种实际开发任务\n如何通过「神马中转 API」连接 GPT-5.1 与 Claude\n"},{"id":13,"href":"/docs/ailearn/model/","title":"基础教程-支持模型 · 页面","section":"站内基础教程","content":" 支持模型页面-各方面解析 # 🔍 智能筛选系统 # 左侧筛选面板 → 勾选条件 → 实时刷新列表\n💰 倍率分析模式 # 📋 批量复制模型名称 # 开启多选-模型卡片勾选-点击复制模型按钮\n📋 模型厂商卡片说明 # "},{"id":14,"href":"/docs/openai/ptdhfl/","title":"普通文本对话（非流）","section":"Openai请求格式（通用）","content":" 普通文本对话（非流） # 基础信息 # 接口地址: https://api.whatai.cc\n认证方式: Bearer Token (API Key)\n普通对话接口 # POST 请求示例\nimport requests import json # 配置API参数 url = \u0026#34;https://api.whatai.cc/v1/chat/completions\u0026#34; # API端点 payload = { \u0026#34;model\u0026#34;: \u0026#34;gpt-4o-mini\u0026#34;, # 指定模型 \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;You are a helpful assistant.\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;周树人和鲁迅是兄弟吗？\u0026#34; } ] } headers = { \u0026#39;Accept\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Authorization\u0026#39;: \u0026#39;sk-******\u0026#39;, # 替换为你的API Key \u0026#39;User-Agent\u0026#39;: \u0026#39;xxx/1.0.0\u0026#39;, \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39; } # 发送请求 response = requests.post(url, headers=headers, data=json.dumps(payload)) print(response.text) 使用OpenAI官方SDK # from openai import OpenAI # 初始化客户端 client = OpenAI( api_key=\u0026#34;sk-******\u0026#34;, # 替换为你的API Key base_url=\u0026#34;https://api.whatai.cc/v1\u0026#34; # API中转地址 ) # 创建对话 chat_completion = client.chat.completions.create( messages=[ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;周树人和鲁迅是兄弟吗？\u0026#34;, } ], model=\u0026#34;o1\u0026#34;, # 指定模型 ) print(chat_completion) 注意事项 # 请妥善保管API Key，不要泄露\n所有请求必须通过HTTPS发送\n响应格式为JSON\n"},{"id":15,"href":"/docs/aidocs/key/","title":"添加令牌：您的AI通行证指南","section":"基础知识","content":" 🔑 添加令牌：您的AI通行证指南 # 一Key在手，AI我有！ 令牌（API Key）是解锁AI能力的数字钥匙，就像手机SIM卡激活网络\n⚠️ 安全警示：\nKey泄露 = 家门大开！ → 立即删除并重建\n🔐 密钥格式：sk-xxxxxxxx（默认隐藏，可点击右边按钮复制）\n💻 操作入口\n👉 神马中转API令牌控制台\n📄 配置项说明 # 🔖1. 令牌名称 # 自定义标识符（建议按用途命名，如\u0026quot;生产环境API\u0026quot;） ⏳ 2. 过期时间 # 设置令牌的有效期间，超过则该令牌无法使用 💰 3. 额度 # 配置当前令牌的可使用额度\n选项 可用额度规则 ✅ 无限额度 动态关联主账户余额，可用额度 = 钱包实时余额 🔄 自定义额度 设定固定当前令牌的消费上限 **若设置值 \u0026gt; 钱包余额时：**实际额度 = 钱包余额 🔢4. 创建数量 # 默认为1\n当您输入数量值 N（如2），系统将创建N（2）个相同（当前）配置项的令牌\n🌐5. 令牌渠道分组 # 🎯 渠道分组配置 # [配置作用]\n来源差异化：不同分组对应不同来源，稳定性高选高价分组，不介意稳定性则可以选择低价分组\n价格差异化：不同分组对应不同计费标准\n消费隔离：将高/低成本业务分流到不同分组\n预算优化：为测试/生产环境令牌配置不同成本分组\n例：开发令牌配置低成本组，生产令牌配置高稳定性组\n⚠️ 注意：请在****页面查看，根据自己实际情况进行配置 [配置规则]\n支持多选分组\n优先级排序规则：\n• 位置越靠前优先级越高\n• 同模型存在多组时 → 优先使用排序第一的组\n[示例]\n分组排序：[default, claude官, origin] → 调用共用模型时优先使用default组渠道\n⚙️自动选择分组机制 # [工作逻辑]\n✅ 开启时：\n系统检测调用模型所属分组\n若模型不在令牌分组中 → 自动重定向到模型默认组\n例：模型a仅在ssvip中有，但是上面渠道分组配置只选了vip组 → 调用模型a时，自动使用ssvip组渠道\n❌ 关闭时：\n调用模型不在令牌分组 → 直接返回403错误\n⚠️ 注意：默认开启，若不需要，需要勾选关闭自动选择分组选项\n🔧6. 模型限制 # ✅ 默认关闭：令牌可访问平台所有模型\n启用时：仅限访问明确指定的模型\n【示例】该令牌只能调用c-3-sonnet、flux-chat模型\n🔧7. 对话模型设置（可配置可不配置） # 1️⃣ 启用图片请求格式整理**(可选)**\n作用：自动修复不规范的识图请求格式（如多图拼接错误、参数缺失等）。\n建议：仅当出现图片识别失败时开启，正常情况无需启用。\n2️⃣ 启用图片链接转 base64 再传递**(可选)**\n作用：将大陆无法访问的图片链接转为Base64编码传递，解决官方接口屏蔽问题。\n注意：会延长响应时间300-800ms，非大陆用户不建议开启。\n3️⃣ 设置推理模型 Reason 内容的位置**(可选)**\n作用：手动指定模型返回推理内容（Reason）的字段位置。\n注意：默认会根据模型返回内容设置，推理内容可能在 Think 、 reason_content 中\n💡 8.模型重定向（可选） # 作用：魔法级请求变身术！ # 自动把请求中的 模型A 变成 模型B，完美解决：\n1️⃣ 老客户端卡在旧模型 → 🤖 原地升级新模型\n2️⃣ 客户端锁死模型 → 🔓 绕过限制器\n3️⃣ 模型突然下架 → 急救重定向\n⚙️ 配置规则： # [输入框1] 客户请求的模型名（伪装名）[输入框2] 真实调用的模型名（替身演员）【示例】客户端请求gpt-3.5，服务器真实调用gpt-4-turbo\n🔒 IP白名单（安全锁） # 启用后 → 只有白名单IP能通行\n其他IP访问？直接吃闭门羹！ 🚫\n【输入框】 填写你的 IP 或IP段（一个逗号一个） 【示例】： 114.214.186.111,182.168.1.0/24\n💥 使用场景\n▸ 企业服务器固定IP → 🤖 禁止员工摸鱼\n▸ 只允许自家APP调用 → 📱 封杀第三方工具\n▸ 金融级安全防护 → 💰 防API密钥泄露\n✅ 救命锦囊：\n先添加当前IP（防自锁）\n公司IP用/24网段覆盖\n开小号令牌做备用钥匙 🔑\n🌟 终极安全套餐：\n【IP白名单】 + 【模型限制】 + 【用量监控】= 黑客看了连夜删号跑路 👻\n⚠️****预警额度 # 设置当前令牌的额度红线 → 自动提醒🔔\n📌 必看重点\n1️⃣ 先到个人中心→消息订阅开启通知❗\n▸ 没订阅=哑炮预警🤫\n【示例】当前令牌剩余算力额度到达 10时提醒\n🚦 限流器（可选） # 当用户 XXX秒内狂点 XXX次 → 直接发射 冷静导弹 💥\n（自动弹出提示 + 强制冷静期 ❄️）\n【示例】当前用户10秒请求该令牌50次，提示你让服务器裂开了！\n🔥 经典套餐\n防刷场景 推荐配置 死亡提示 防脚本爆破 5秒10次 \u0026ldquo;人类，你太快了！\u0026rdquo; 🤖 保护GPT-4钱包 60秒3次 \u0026ldquo;手慢点！烧的是钱啊！\u0026rdquo; 💸 🎨 MJ绘图模式 # 给每个令牌绑定独立绘图策略\n令牌纬度控制 Midjouney 配置，设置优先级：令牌 \u0026gt; 路径参数 \u0026gt; 系统默认\n🌐 MJ图片代理方案（速度救星） # 控制在 Fetch、回调中的图片地址(image_url)\n选项 效果 总结 系统默认 随缘加载，可能转圈到崩溃 🌀 🐢💔 (龟速心碎) (origin) Discord （直连源地址） 国外很快，国内无法访问 🌏❌ (地球打叉) (proxy) 图片代理地址 国外很慢，国内可以访问 🌏❌ (地球打叉) 自定义图片代理 自建高速通道，爽到起飞 🚀✨ (火箭喷发) 令牌纬度控制 Midjouney 配置，设置优先级：令牌 \u0026gt; 路径参数 \u0026gt; 系统默认\n⚠️ 异步任务 · 关闭资源代理 # 🚫 高危操作警告\n关闭此开关 = 亲手拆除安全气囊 🛡️💥\n系统将不再自动修复以下问题： 国内访问 Discord 必失败的墙问题 🧱\n浏览器跨域拦截(CORS) ❌\n需要自行解决\n"},{"id":16,"href":"/docs/otherai/tools/dify/","title":"Dify教程","section":"工具类","content":"\nDify 是一款开源的大语言模型（LLM）应用开发平台。它融合了后端即服务（Backend as Service）和 LLMOps 的理念，使开发者可以快速搭建生产级的生成式 AI 应用。即使你是非技术人员，也能参与到 AI 应用的定义和数据运营过程中。\n由于 Dify 内置了构建 LLM 应用所需的关键技术栈，包括对数百个模型的支持、直观的 Prompt 编排界面、高质量的 RAG 引擎、稳健的 Agent 框架、灵活的工作流，并同时提供了一套易用的界面和 API。这为开发者节省了许多重复造轮子的时间，使其可以专注在创新和业务需求上。\n一、为什么使用 Dify？ # 你或许可以把 LangChain 这类的开发库（Library）想象为有着锤子、钉子的工具箱。与之相比，Dify 提供了更接近生产需要的完整方案，Dify 好比是一套脚手架，并且经过了精良的工程设计和软件测试。 重要的是，Dify 是开源的，它由一个专业的全职团队和社区共同打造。你可以基于任何模型自部署类似 Assistants API 和 GPTs 的能力，在灵活和安全的基础上，同时保持对数据的完全控制。\n二、你能用 Dify 做什么 # 快速搭建问答机器人（基于企业文档的 RAG）\n构建会调用工具（搜索、调用 API、生成图片等）的 Agent（自动化任务）\n在可视化画布上用“节点 + 触发 + 条件”编排复杂流程（Workflow）\n通过 REST API 将这些能力嵌入现有业务系统（后端代理调用）\n（上面功能点与细节可在官方产品文档与代码库查看）。\n三、准备工作（最小环境 \u0026amp; 工具） # 最低硬件（参考仓库说明）：CPU ≥ 2 core，RAM ≥ 4 GiB（用于快速试验）。生产部署请按负载规划。\n软件：Docker、Docker Compose（本教程以 Docker Compose 快速启动为例）；如果要生产部署可使用 Helm / Kubernetes。\n四、快速入门 — 本地（Docker Compose）部署（最短命令） # # 克隆仓库并启动（在支持 Docker 的机器上） git clone https://github.com/langgenius/dify.git cd dify/docker cp .env.example .env # 编辑 .env（填写管理员邮箱、数据库/MinIO/模型供应商的凭证等） docker compose up -d # 启动后到 http://localhost/install 进行初始化 上述为官方 quick start 的核心命令；具体 .env 字段和更高级部署（K8s/Helm/Terraform/AWS/CDK）见官方文档与仓库说明。\n注意\n.env.example 里会列出诸如数据库、存储（MinIO）、SMTP、初始管理员账号等变量，生产环境请使用安全凭证并放到安全 Vault。 五、核心概念与流程（快速理解） # Model Provider（模型供应商）：Dify 支持大量供应商（OpenAI、Anthropic、Azure OpenAI、Gemini、Hugging Face、Mistral、Replicate、Ollama、LocalAI 等），可以把外部模型或自部署模型接入到平台并供应用调用。\nKnowledge（知识库 / RAG）：把文档/PDF/网页/数据库转成可检索的向量/片段，查询时先检索相关片段再交给 LLM 生成，减少“幻觉”。\nWorkflow / Agent：Workflow 是显式的节点流程（可视化编排），Agent 则是“策略驱动”的智能体（例如 Function Calling / ReAct），可动态选择并调用工具。\nLLMOps / 监控：Dify 提供日志、metric 与标注能力，便于持续优化 prompts、模型与数据。\n六、使用神马中转API（国内直连） # 1.使用OpenAI模型 # 点击右上角自己的头像，点击设置\n点击模型供应商，选择OpenAI的设置\n输入神马中转API 后台-使用API-API Keys 生成的API KEY，输入API Base：https://api.whatai.cc/v1/chat/completions，点击保存即可 不同的版本尝试填入不同的API地址： https://api.whatai.cc https://api.whatai.cc/v1 https://api.whatai.cc/v1/chat/completions 2. 使用其他模型 # 下拉找到OpenAI-API-compatible，选择添加模型\n模型名称填写你想要使用的模型，准确的名称可以通过 模型价格页面 查看 输入神马中转AI 后台-使用API-API Keys 生成的API KEY，输入API Base：https://api.whatai.cc/v1/chat/completions，点击保存即可\n不同的版本尝试填入不同的API地址： https://api.whatai.cc https://api.whatai.cc/v1 https://api.whatai.cc/v1/chat/completions 七、构建知识库 (RAG) — UI 与 API 示例 # 用 UI （最快） # Dashboard -\u0026gt; Knowledge -\u0026gt; 创建知识库 -\u0026gt; 上传文件 / 粘贴文本 -\u0026gt; 选择索引策略（例如 high_quality / economical 等） -\u0026gt; 触发索引。 用 API（适合自动化 / 批量） # 示例：通过文本创建文档（curl）：\ncurl --location --request POST \u0026#39;https://api.dify.ai/v1/datasets/{dataset_id}/document/create_by_text\u0026#39; \\ --header \u0026#39;Authorization: Bearer {api_key}\u0026#39; \\ --header \u0026#39;Content-Type: application/json\u0026#39; \\ --data-raw \u0026#39;{\u0026#34;name\u0026#34;: \u0026#34;company_faq\u0026#34;,\u0026#34;text\u0026#34;: \u0026#34;这是一个关于产品的常见问题集合...\u0026#34;,\u0026#34;indexing_technique\u0026#34;: \u0026#34;high_quality\u0026#34;,\u0026#34;process_rule\u0026#34;: {\u0026#34;mode\u0026#34;: \u0026#34;automatic\u0026#34;}}\u0026#39; 上传文件示例（curl）：（先上传文件或直接用 create_by_file）\ncurl --location --request POST \u0026#39;https://api.dify.ai/v1/datasets/{dataset_id}/document/create_by_file\u0026#39; \\ --header \u0026#39;Authorization: Bearer {api_key}\u0026#39; \\ --form \u0026#39;data=\u0026#34;{\\\u0026#34;indexing_technique\\\u0026#34;:\\\u0026#34;high_quality\\\u0026#34;,\\\u0026#34;process_rule\\\u0026#34;:{\\\u0026#34;mode\\\u0026#34;:\\\u0026#34;custom\\\u0026#34;}}\u0026#34;;type=text/plain\u0026#39; \\ --form \u0026#39;file=@\u0026#34;/path/to/file.pdf\u0026#34;\u0026#39; （更多 API 列表、分段/元数据管理、查询接口参见知识库 API 文档页面）\n八、在 Workflow / Agent 里使用知识检索（典型流程） # 在 Workflow 画布放置 知识检索（Knowledge Retrieval） 节点，配置要检索的知识库与检索参数（top-k、元数据过滤等）。\n将知识检索节点的输出作为 LLM 节点的上下文输入（LLM 节点接收到用户 query + 检索到的片段）。\n如果需要自动化任务（如：查询网站、调用计算工具、生成图片等），使用 Agent 节点 并为其配置 Agent 策略（Function Calling / ReAct）与所需的 Tools（工具）。Dify 提供大量内置工具（搜索、图片生成、计算、外部 API 等），也支持导入 OpenAPI/Plugin 格式的自定义工具。\n九、发布应用并通过 API 调用（示例） # 获取 API Key 与发布：在应用 -\u0026gt; 访问 API 页面生成访问凭据（注意安全。API Key 请只放后端使用，不要写在前端）。\n示例：调用聊天接口（curl）\ncurl --location --request POST \u0026#39;https://api.dify.ai/v1/chat-messages\u0026#39; \\ --header \u0026#39;Authorization: Bearer ENTER-YOUR-SECRET-KEY\u0026#39; \\ --header \u0026#39;Content-Type: application/json\u0026#39; \\ --data-raw \u0026#39;{ \u0026#34;inputs\u0026#34;: {}, \u0026#34;query\u0026#34;: \u0026#34;请帮我总结一下我上传的产品文档里关于退款政策的要点。\u0026#34;, \u0026#34;response_mode\u0026#34;: \u0026#34;streaming\u0026#34;, \u0026#34;conversation_id\u0026#34;: \u0026#34;your-conv-id\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;user-123\u0026#34; }\u0026#39; Python 示例（requests）\nimport requests url = \u0026#34;https://api.dify.ai/v1/chat-messages\u0026#34; headers = {\u0026#34;Authorization\u0026#34;: \u0026#34;Bearer YOUR_API_KEY\u0026#34;, \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;} data = { \u0026#34;inputs\u0026#34;: {}, \u0026#34;query\u0026#34;: \u0026#34;请列出产品 A 的主要功能\u0026#34;, \u0026#34;response_mode\u0026#34;: \u0026#34;sync\u0026#34; } r = requests.post(url, headers=headers, json=data) print(r.status_code, r.json()) 官方 API 文档有更多 endpoint（completion-messages、chat-messages、工作流调用、文件上传等）与示例。\n十、进阶：自定义插件 / 新模型 / 工具 # 自定义工具（OpenAPI / OpenAI plugin）：在 Tools -\u0026gt; 自定义，创建可供 Agent 调用的外部服务（例如内部 CRM API）。\n新增模型供应商 / 插件：使用 Dify 的插件脚手架可以开发 model/provider 插件（需要 Python 环境，参见插件开发文档）。这允许把厂内自研推理服务接入 Dify。\n十一、常见问题与排错技巧（实用） # 启动后无法访问 /install：检查 Docker container 是否运行，查看 docker compose ps，并查看容器日志 docker compose logs -f。\nAPI 请求示例缺少 base URL 导致 404/错误：文档中的部分 curl 片段可能只给出相对路径，调用前请加上完整的 Base URL（例如 h ttp://localhost/v1/\u0026hellip; 或 h ttps://api.dify.ai/v1/\u0026hellip;）。有开发者因此报过相关问题。\nApp unavailable / 400：确保 App 已正确初始化、配置了模型供应商并启用了相应功能；在社区 issue 有类似错误的排查建议。\n十二、完整示例：从零创建一个 RAG 问答机器人（快速脚本） # 部署 Dify（参考第 4 节）。\n在 Dashboard -\u0026gt; Knowledge -\u0026gt; 新建知识库（或用 API 创建 POST /v1/datasets）。\n用 API 上传文档（示例见第 7 节的 create_by_file）。\n在 Workflow 画布：加入 知识检索节点（指向上面的知识库）→ LLM 节点（选择已配置的模型）→ 将检索结果拼接进 prompt。\n本地测试并调整 prompt 与检索 top-k，确认正确后发布应用并生成 API key。\n"},{"id":17,"href":"/docs/otherai/chat/lobechat/","title":"Lobe-Chat","section":"对话客户端类","content":" Lobe-Chat ​配置教程 # 将接口代理地址改成API地址的链接：https://api.whatai.cc/v1 或者 https://api.whatai.cc\n"},{"id":18,"href":"/docs/otherai/devtools/codegpt/","title":"VSCode插件Code GPT使用指南","section":"编程工具类","content":" VSCode 插件 Code GPT 使用指南 # 这个插件修改 Host 相对麻烦一些，需要修改源码才可以使用。以下是详细的步骤：\n步骤 # 这个插件修改Host相对麻烦一些，需要修改源码才可以使用。\n安装插件。安装好后按Ctrl+Shift+P，弹出框中输入Open Extensions Floder 点击Extensions: Open Extensions Floder，这将打开插件目录，找到Code GPT的文件夹。 打开后进入打开文件./src/clients/openai_client.js，搜索文件中的api.openai.com，并替换为 api.whatai.cc。保存文件。 再次回到vscode，按Ctrl+Shift+P，弹出框中输入CodeGPT: Set API KEY，点击CodeGPT: Set API KEY。然后将购买的Key输入进去即可。 以上步骤完成后，重启VSCode 其他VSCode插件类似。 "},{"id":19,"href":"/docs/aidocs/api/","title":"基本概念-API地址（BaseURL）","section":"基础知识","content":" 📖基本概念-API地址（BaseURL） # 1️⃣ 基本概念 # 中转网址，用于替换官方API地址\n官方地址：https://api.openai.com 需要被替换成 神马中转API地址：https://api.whatai.cc\n所有地址数据互通\n💻 API（BaseUrl）信息入口： # 👉 神马中转API工作台\n根据自己调用API的地理位置选择最优线路以获得最佳体验\n部分路线可能区域网络受限，若无法正常访问API地址，请尝试更换地址\n🔌 完整功能接口映射表 # 功能 路径 完整示例URL 智能对话 /v1/chat/completions https://api.whatai.cc/v1/chat/completions 文本嵌入 /v1/embeddings https://api.whatai.cc/v1/embeddings AI绘图 /v1/images/generations https://api.whatai.cc/v1/images/generations 语音转文本 /v1/audio/transcriptions https://api.whatai.cc/v1/audio/transcriptions 文本转语音 /v1/audio/speech https://api.whatai.cc/v1/audio/speech 在 claude code 里只需要配置域名，不需要带 /v1及后面的部分。\nPython 示例\n需要先 pip install openai\nimport openai # 配置API密钥和基础URL openai.api_key = \u0026#34;sk-******\u0026#34; # 替换为你的实际令牌 openai.api_base = \u0026#34;https://api.whatai.cc\u0026#34; # 设置中转地址# 调用对话接口示例 response = openai.ChatCompletion.create( model=\u0026#34;gpt-3.5-turbo\u0026#34;, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;你好\u0026#34;}] ) print(response.choices[0].message.content) curl 示例 # curl --request POST \\ --url https://api.whatai.cc/v1/chat/completions \\ --header \u0026#39;Authorization: Bearer sk-替换为你的key\u0026#39; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ --data \u0026#39;{ \u0026#34;max_tokens\u0026#34;: 8192, \u0026#34;model\u0026#34;: \u0026#34;gpt-4.1-mini\u0026#34;, \u0026#34;temperature\u0026#34;: 0.8, \u0026#34;top_p\u0026#34;: 1, \u0026#34;presence_penalty\u0026#34;: 1, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;你是我的全能助手，你叫小美\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;你是谁？夸我几句我就给你续费~\u0026#34; } ] }\u0026#39; "},{"id":20,"href":"/docs/ailearn/chat/","title":"基础教程-聊天 · 页面","section":"站内基础教程","content":" 基础教程-聊天 · 页面 # ⚡ 双通道配置聊天 ​ # ⚠️ 请先创建令牌，才可使用站内聊天\n💻管理入口👉 令牌控制台\n📖令牌配置指南👉 基础知识-基本概念-添加令牌\n🚀 方式一：闪电直连（推荐） # 在令牌列表找到目标Key\n点击「聊天」按钮\n系统自动完成：\n注入Key到聊天窗\n配置BASE_URL\n选择默认模型\n💡 适合场景：快速体验、临时调试\n🛠️ 方式二：手动配置（高级） # Step 1 进入聊天页面 → 设置 ⚙️ → 服务端\nStep 2 填写关键参数：\n必备参数： API Key(令牌) ███ sk-xxxxxxxxxxxx （粘贴复制的Key） BaseURL ███ 选填以下任一： https://api.whatai.cc https://api.whatai.cc/v1 https://api.whatai.cc/v1/chat/completions Midjourney专用地址： https://api.whatai.cc https://api.whatai.cc/mj BaseURL指南：基础知识-基础概念-API地址（BaseURL） Step 3 填写模型：\n访问「支持模型」页面\n复制目标模型名称（如 gpt-4-turbo）\n在聊天窗口粘贴到模型选择框\n4.点击保存\nStep 4 回到聊天页面，可以开始聊天啦！！！！！！！\n"},{"id":21,"href":"/docs/openai/ptdhls/","title":"普通文本对话（流式输出）","section":"Openai请求格式（通用）","content":" 普通文本对话（流式输出） # 基础信息 # 接口地址: https://api.whatai.cc\n认证方式: Bearer Token (API Key)\n请求参数 # 参数 类型 说明 model string 模型名称(如gpt-4o-mini) stream boolean 是否开启流式输出 messages array 对话消息列表 Python 请求示例 # import json import requests # 配置请求参数 url = \u0026#34;https://api.whatai.cc/v1/chat/completions\u0026#34; headers = { \u0026#34;Authorization\u0026#34;: \u0026#34;Bearer sk-******\u0026#34;, # 替换为你的API Key \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34; } payload = { \u0026#34;model\u0026#34;: \u0026#34;gpt-4o-mini\u0026#34;, \u0026#34;stream\u0026#34;: True, # 开启流式输出 \u0026#34;messages\u0026#34;: [ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;You are a helpful assistant.\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;周树人和鲁迅是兄弟吗？\u0026#34;}, ] } # 发送请求并处理流式响应 response = requests.post(url, headers=headers, json=payload, stream=True) buffer = \u0026#34;\u0026#34; for chunk in response.iter_content(chunk_size=None): if chunk: buffer += chunk.decode(\u0026#34;utf-8\u0026#34;) while \u0026#34;\\n\u0026#34; in buffer: line, buffer = buffer.split(\u0026#34;\\n\u0026#34;, 1) if not line.strip(): # 跳过空行 continue if line.startswith(\u0026#34;data: \u0026#34;): data_line = line[6:].strip() # 去除\u0026#34;data: \u0026#34;前缀 if data_line == \u0026#34;[DONE]\u0026#34;: # 流式结束标记 break try: data = json.loads(data_line) # 提取并打印响应内容 content = data[\u0026#34;choices\u0026#34;][0][\u0026#34;delta\u0026#34;].get(\u0026#34;content\u0026#34;, \u0026#34;\u0026#34;) print(content, end=\u0026#34;\u0026#34;, flush=True) except json.JSONDecodeError: # 处理不完整JSON数据 buffer = line + \u0026#34;\\n\u0026#34; + buffer break 注意事项 # 请妥善保管API Key\n建议使用HTTPS协议\n响应格式为JSON\n"},{"id":22,"href":"/docs/otherai/devtools/easycode/","title":"JetBrains插件Easycode​","section":"编程工具类","content":" Jetbrains插件ChatGPT - Easycode # 安装好插件后在Settings \u0026gt; Tools \u0026gt; OpenAI \u0026gt; GPT 3.5 Turbo中如图所示配置好插件，重点要将Server Settings 修改为 https://api.whatai.cc/v1/chat/completions 。并勾选Customize Server。\n"},{"id":23,"href":"/docs/otherai/chat/utools-chatgpt/","title":"utools-ChatGPT","section":"对话客户端类","content":" utools-ChatGPT配置教程 # "},{"id":24,"href":"/docs/openai/gdgshscjson/","title":"固定格式化输出Json","section":"Openai请求格式（通用）","content":" 固定格式化输出Json # 格式化输出 接口说明 # 通过OpenAI API获取产品信息，返回JSON格式数据。\n请求地址 # POST https://api.whatai.cc/v1/chat/completions\n请求参数 # Headers # 参数名 类型 必填 说明 Authorization string 是 API密钥，格式: Bearer ****** Content-Type string 是 固定值: application/json Body # { \u0026#34;model\u0026#34;: \u0026#34;gpt-4o-2024-08-06\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;根据给出的产品进行分析，按json格式用中文回答,json format:product_name, price, description.\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;产品描述\u0026#34; } ], \u0026#34;response_format\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;json_object\u0026#34; } } Python示例代码 # from pydantic import BaseModel from openai import OpenAI from dotenv import load_dotenv import json from textwrap import dedent # 加载环境变量，例如 API key 等配置信息 load_dotenv() # 设置 OpenAI API 的工厂名称，默认为 \u0026#34;openai\u0026#34; factory = \u0026#34;openai\u0026#34; # 初始化 OpenAI 客户端，传入 API key 和 base URL client = OpenAI( api_key=\u0026#34;sk-*******************************\u0026#34;, # 替换为你的 API key base_url=\u0026#34;https://api.whatai.cc/v1/\u0026#34; # 这里是 base url，注意这里需要 /v1/ ) # 定义一个产品信息类，用于解析 API 返回的数据 class ProductInfo(BaseModel): product_name: str # 产品名称，字符串类型 price: float # 价格，浮点数类型 description: str # 产品描述，字符串类型 # 定义一个提示信息，用于请求模型返回 JSON 格式的产品信息 product_prompt = \u0026#39;\u0026#39;\u0026#39;根据给出的产品进行分析，按json格式用中文回答,json format:product_name, price, description.\u0026#39;\u0026#39;\u0026#39; # 获取产品信息的函数，传入用户的问题 def get_product_info(question: str): # 使用 OpenAI 客户端进行聊天模型的请求 completion = client.beta.chat.completions.parse( model=\u0026#34;gpt-4o-2024-08-06\u0026#34;, # 指定使用的模型 messages=[ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: dedent(product_prompt)}, # 发送系统消息，设置模型的行为 {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: question}, # 发送用户消息，用户提出问题 ], response_format=ProductInfo, # 指定返回的数据格式为 ProductInfo ) # 返回模型解析的第一个选项的消息结果 return completion.choices[0].message.parsed # 初始化一个空的产品信息字典 product_inform = {} # 定义将解析的结果转换为 JSON 的函数 def transform2JSON(parsed_result): # print(parsed_result) # 打印解析结果 # 将解析的结果存储到字典中 product_inform[\u0026#34;product_name\u0026#34;] = parsed_result.product_name product_inform[\u0026#34;price\u0026#34;] = parsed_result.price product_inform[\u0026#34;description\u0026#34;] = parsed_result.description # 将字典转换为 JSON 字符串并返回，ensure_ascii=False 允许中文字符正常显示 return json.dumps(product_inform, ensure_ascii=False, indent=4) # 定义用户输入的问题，即一个产品信息的描述 question = \u0026#34;75寸小米电视机\u0026#34; # 调用函数获取产品信息 result = get_product_info(question) # 将解析结果转换为 JSON 格式并打印 json_result = transform2JSON(result) print(json_result) 返回示例 # { \u0026#34;product_name\u0026#34;: \u0026#34;小米电视75寸\u0026#34;, \u0026#34;price\u0026#34;: 4999.0, \u0026#34;description\u0026#34;: \u0026#34;4K超高清画质，支持HDR，内置小爱同学语音助手\u0026#34; } 注意事项 # 请妥善保管API密钥\n产品信息由AI生成，仅供参考\n"},{"id":25,"href":"/docs/aidocs/group/","title":"基本概念-令牌渠道分组","section":"基础知识","content":" 📖 基本概念-令牌渠道分组 # 1️⃣ 基本概念 # 别称：分组、渠道、渠道分组、API分组\n区分模型的渠道来源和对应的价格\n价格透明**·**用户可以根据自己的需求，选择最适合自己的分组。\n⚠️ 不同分组只是个别模型价格不一样，不是所有模型都会价格不一样。\n使用方式： 在令牌页面→新增/编辑令牌时，选择对应的渠道分组即可\n🌲 渠道分组使用教程：\n💻 模型分组详情查看入口：\n💰 官方费率解析 # 当官方费率 = 1 时，网站上1算力额度等价于对应的模型官方价格的 1$。\n当官方费率 = 2 时，网站上1算力额度等价于对应的模型官方价格的 0.5 $。\n📊 API分组游乐场（选对分组，效果翻倍！） # 分组名 渠道来源 必杀技 适合人群标签 默认分组（deafult）·万能工具包 GPT+Claude混合 全能模型全覆盖 想一次性试用所有模型的新手玩家 claude官 · 特供快线 Claude官方 官方特价（偶尔排队） 想薅Claude羊毛的精明党 openai官-优质 · 超跑VIP赛道 OpenAI纯净 o1/realtime加速 🚀速度狂魔·科技发烧友 origin · 头等舱服务 官方原价 ✨ 企业级稳定性（稳定性堆到满级，不差钱！） 💼土豪公司/拒绝翻车的严苛需求 AZ-优质 · 性价比之王 Azure纯净 o1/o3加速 ️性能与预算平衡大师 svip · 省钱宝 Azure纯净 极致低价 💰预算敏感型 vvip · 长文本刺客 OpenAI逆向 大段文字优惠 📜论文党·小说作家 vip · 用量忍者 OpenAI逆向 按量付费越用越便宜 📊灵活用量派 claude · 逆神秘通道 Claude逆向 Cursor专属优惠 🤫逆向探索家 国产特价 · 清仓大促 国产模型 骨折价限时供应手慢无 🛒捡漏王·短期项目党 gemini优质 · 视频解说员 Gemini Pro 支持视频解析🎥 🎬短视频创作者·多媒体控 ✨ 分组选择攻略： # 要速度 → 选「超跑VIP赛道」(openai官-优质)\n要省钱 → 蹲「特价独享通道」(claude官)\n要稳定 → 砸钱上「头等舱」(origin)\n玩长文 → 认准「长文本刺客」(vvip)\n💡 比喻总结：默认分组=共享单车 | 特价分组=打折高铁 | 企业分组=私人飞机就像选交通工具——根据你的预算和目的地，挑最对的那辆！\n✨ 渠道来源说明（看不懂的话可忽略）： # 渠道来源 技术本质 超能力✨ 小缺陷⚠️ 最适合谁 逆向 OpenAI官网逆向 🧠 官网同款智商 💰 价格优惠 🔒 非官方接口 预算有限的聪明玩家 按次混合 逆向+Azure双引擎 ⚡️ 双重加速 💸 用多少次付多少 🔒 非官方接口 用量不固定的灵活派 Azure 微软官方通道 支持100w并发 🛠️ FC/TC全功能 偶尔降速 🔍 有内容审核 企业正规军 企业无审核 无审核Azure+OpenAI兜底 🌪️ 1000w超高并发 🗽 畅所欲言无审核 ⚡ 双引擎护航 ⛈️ 偶尔天气干扰 土豪公司/敏感内容需求 纯官方 官方直连 ✨ 官网原生体验 🌪️ 1000w超高并发 ⚡ 极速响应 🌐 受官网波动影响 极致体验追求者 ✨ 模型不同渠道不同分组说明（看不懂的话可忽略）： # 类型 逆向 按次计费（混合） Azure 企业无审核 纯官方（现在默认分组） GPT-4 逆向，全部支持 逆向，全部支持 Azure，全部支持 全部支持 OpenAl，全部支持 GPT-3.5 逆向，全部支持 逆向，全部支持 Azure，全部支持 全部支持 OpenAl，全部支持 GPTs. All 全部支持，按 token计费 全部支持，按次计费 全部支持 全部支持 全部支持 OpenAI 其他 基础模型 全部支持 全部支持 全部支持 全部支持 Midjourney 全部支持 全部支持 全部支持 全部支持 全部支持 国产模型 全部支持 全部支持 全部支持 全部支持 全部支持 Claude 逆向，全部支持 逆向，全部支持 纯 AWS Claude，全部支持 纯 Claude 官方，全部支持 纯 Claude 官方，全部支持 其他模型 全部支持 全部支持 全部支持 全部支持 全部支持 tools call. function call 不支持 不支持 全部支持 全部支持 全部支持 类型 逆向 按次计费（混合） Azure 企业无审核 纯官方（现在默认分组） 分组 vip vvip svip default ssvip 4.0系列 (包含32k、Dalle3) 官方费率 * 0.8 0.1$ 一次 官方费率 官方费率 * 2 官方费率 * 2.5 3.5系列 官方费率 * 0.2 0.005$ 一次 官方费率 官方费率 * 2 官方费率 * 2.5 Claude 逆向 Anthropic Claude Google/AWS Claude Google/AWS Claude Anthropic Claude Gemini 普号号池（便宜，不支持高并发） 普号号池（便宜，不支持高并发） 普号号池（便宜，不支持高并发） 普号号池（便宜，不支持高并发） 官方付费版（支持高并发） Luma API 0.3$ 一次，不可商用 0.3$ 一次，不可商用 0.6$ 一次，可商用，排队优先 0.6$ 一次，可商用，排队优先 0.6$ 一次，可商用，排队优先 其他模型 所有分组统一 所有分组统一 所有分组统一 所有分组统一 所有分组统一 "},{"id":26,"href":"/docs/ailearn/dev/","title":"基础教程-开发者快速接入","section":"站内基础教程","content":" 基础教程-开发者 · 快速接入 # 基础信息 # Base URL: https://api.whatai.cc\n认证方式: Bearer Token (API Key)\n更多baseURL 👉 基础概念-API地址\n文本对话接口 # 请求\n方法: POST\n路径: /v1/chat/completions\nHeaders:\nAccept: application/json\nAuthorization: Bearer sk-****** (你的API Key)\nContent-Type: application/json\n请求参数 # { \u0026#34;model\u0026#34;: \u0026#34;gpt-4o-mini\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;You are a helpful assistant.\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;周树人和鲁迅是兄弟吗？\u0026#34; } ] } cURL 示例 # curl -X POST \u0026#34;https://api.whatai.cc/v1/chat/completions\u0026#34; \\ -H \u0026#34;Accept: application/json\u0026#34; \\ -H \u0026#34;Authorization: Bearer sk-******\u0026#34; \\ # 替换为你的API Key -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;gpt-4o-mini\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;You are a helpful assistant.\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;周树人和鲁迅是兄弟吗？\u0026#34; } ] }\u0026#39; Python 示例 # import requests import json # API配置 url = \u0026#34;https://api.whatai.cc/v1/chat/completions\u0026#34; headers = { \u0026#39;Accept\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Authorization\u0026#39;: \u0026#39;Bearer sk-******\u0026#39;, # 替换为你的API Key \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39; } # 请求数据 payload = json.dumps({ \u0026#34;model\u0026#34;: \u0026#34;gpt-4o-mini\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;You are a helpful assistant.\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;周树人和鲁迅是兄弟吗？\u0026#34; } ] }) # 发送请求 response = requests.post(url, headers=headers, data=payload) # 输出响应 print(response.text) 响应 # 返回JSON格式的对话结果\n提示：将示例中的sk-******替换为你实际的API Key即可使用\n"},{"id":27,"href":"/docs/openai/embedding/","title":"embedding代码例子","section":"Openai请求格式（通用）","content":" embedding代码例子 # 基本概念 # 什么是Embedding？ # Embedding（嵌入）是将离散数据（如单词、句子）映射到连续向量空间的技术。通过Embedding：\n语义相似的文本在向量空间中距离更近\n便于机器学习模型处理文本数据\n典型应用：搜索、推荐、分类等场景\n常见Embedding模型 # text-embedding-3-small Openai主流emb模型\ntext-embedding-3-large\ntext-embedding-ada-002\n技术特点 # 特性 说明 维度 通常为数百到数千维（如1024维） 归一化 多数Embedding会做L2归一化 距离度量 常用余弦相似度计算向量距离 多语言支持 现代模型通常支持多语言嵌入 请求地址 # POST https://api.whatai.cc/v1/embeddings\n认证方式 # 需要在请求头中添加 API Key：\nheaders = { \u0026#34;Authorization\u0026#34;: \u0026#34;Bearer sk-******\u0026#34;, # 替换为你的API令牌 \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34; } 请求参数 # 参数名 类型 必填 说明 input string/array 是 单条文本或文本列表 model string 是 使用的模型名称 encoding_format string 否 返回格式（float/base64） 请求示例 # import openai # 设置OpenAI API密钥和基础URL openai.api_key = \u0026#34;sk-***************************\u0026#34; # 替换为你的 key openai.base_url = \u0026#34;https://api.whatai.cc/v1/\u0026#34; # 这里是API的 base url，注意这里v1后面需要/，最后的 / 很容易漏掉。 def get_embedding(text): response = openai.embeddings.create( model=\u0026#34;text-embedding-3-small\u0026#34;, input=text ) return response.data[0].embedding # 示例文本 text = \u0026#34;这是一个示例文本,用于演示如何获取文本嵌入。\u0026#34; # 获取文本嵌入 embedding = get_embedding(text) print(f\u0026#34;文本: {text}\u0026#34;) print(f\u0026#34;嵌入向量维度: {len(embedding)}\u0026#34;) print(f\u0026#34;嵌入向量前5个元素: {embedding[:5]}\u0026#34;) 典型应用场景 # 语义搜索：通过向量相似度匹配查询和文档\n聚类分析：将相似文本自动归类\n推荐系统：寻找内容相似的物品\n异常检测：识别语义异常的文本\n性能优化建议 # 批量处理文本（最多支持2048 tokens/请求）\n对静态内容缓存嵌入结果\n使用近似最近邻(ANN)算法加速搜索\n响应示例 # 成功响应(200):\n{ \u0026#34;object\u0026#34;: \u0026#34;list\u0026#34;, \u0026#34;data\u0026#34;: [ { \u0026#34;object\u0026#34;: \u0026#34;embedding\u0026#34;, \u0026#34;embedding\u0026#34;: [0.1, -0.2, 0.3, ...], \u0026#34;index\u0026#34;: 0 } ], \u0026#34;model\u0026#34;: \u0026#34;text-embedding-ada-002\u0026#34;, \u0026#34;usage\u0026#34;: { \u0026#34;prompt_tokens\u0026#34;: 5, \u0026#34;total_tokens\u0026#34;: 5 } } 错误响应:\n{ \u0026#34;error\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;Invalid input text\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;invalid_request_error\u0026#34; } } "},{"id":28,"href":"/docs/otherai/devtools/langchain/","title":"LangChain​","section":"编程工具类","content":" LangChain # 注意事项 ​ # openai_api_base, 的末尾要加上, /v1/chat/completions, ，而且目前只支持 Chat 模型，请确认不要导入错误的包。 示例代码 ​ # from langchain.chat_models import ChatOpenAI llm = ChatOpenAI( openai_api_base=\u0026#34;https://api.whatai.cc/v1/chat/completions\u0026#34;, openai_api_key=\u0026#34;sk-xxxxx\u0026#34;, ) res = llm.predict(\u0026#34;hello\u0026#34;) print(res) "},{"id":29,"href":"/docs/ailearn/sdk/","title":"高级功能-官方SDK支持","section":"站内基础教程","content":" 高级功能-官方SDK支持 # 支持的厂商 # OpenAI SDK\nAnthropic Claude SDK\nGoogle Gemini SDK\n配置方法 # 只需修改 SDK 的 Key 和 Base Url 即可使用\n# 初始化 OpenAI 客户端 client = OpenAI( api_key=\u0026#34;sk-********************************\u0026#34;, # 替换为你的神马中转api 令牌key base_url=\u0026#34;https://api.whatai.cc/v1\u0026#34;, # 使用 神马中转API 中转地址 ) # 示例调用 response = client.chat.completions.create( model=\u0026#34;gpt-4.1-mini\u0026#34;, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Hello!\u0026#34;}] ) print(response.choices[0].message.content) 提示：将 ****** 替换为你实际的 API Key，其他厂商 SDK 配置方式类似\n"},{"id":30,"href":"/docs/aidocs/pay1/","title":"基本概念-按量付费","section":"基础知识","content":" 📖 基本概念-按量付费 # 💸 按量计费：像交水电费一样简单！ # 用多少字，付多少钱！ 就像家里用电——开灯时间越长，电费越多～\n计费示例 ​ # 文字类模型的计费通常是按照 tokens 计算。\n例如，1K tokens 消耗 xx 美金，这种计费方式与官方渠道相同。\n优惠政策 ​ # 官方渠道的美金购买需要实时汇率（大约 1:7）。\n在我们平台，您可以直接以折扣价格购买美金。\n具体汇率请查询充值页。\n🌰 实景案例 # 【示例】对于default分组下的gpt-4o-mini模型，假设提问文字数量为6323 tokens，文字输出数量为2481 tokens\nInput（提示token数）：0.15/M tokens （您提问的文字） Output（补全token数）: 0.6/M tokens （AI回答的文字） Cache（模型倍率）：0.075 计算方式为： $0.15 / 1000000 * 6323 + 0.6 / 1000000 * 2481 + 0.075/1000000 * 2048 = 0.002284$ 📊 模型倍率（看不懂可以忽略） # 支持价格页面的模型价格即实价！\n倍率不是额外收费，而是定价的「基因公式」——就像咖啡店的菜单，拿铁价格= (咖啡豆成本×倍率)，您看到的已是最终价 ✅\n倍率运作原理（三步透明机制） # ⚠️ 下面只是举例子不是实际定价 # 第一步：设定基础价（全球标准） 例如：GPT-4 基础价 = $0.03/千字（输入） # 第二步：乘以模型倍率（难度系数） 例如：倍率设置 1.5倍 → $0.03 × 1.5 = $0.045 # 第三步：生成最终标价（您看到的数字） 菜单直接显示：GPT-4输入 → $0.045/千字 消耗时直接按此计算，无隐藏乘除 ❓ 用户高频疑问解密 # Q：倍率会在日志中单独显示吗？\n→ A：不会！您的消费记录只显示 最终标价 × 用量（如：$0.045 × 50,000字）\nQ：为什么不同模型倍率不同？\n→ A：就像不同车型油耗不同\n文本模型 = 经济轿车（倍率1.0）\n多模态模型 = 越野车（倍率2.0，动力更强）\n高速模型 = 跑车（倍率1.5，为速度付费）\nQ：如何知道某个模型的倍率？\n→ A：在模型价格页面勾选倍率即可\n"},{"id":31,"href":"/docs/otherai/chat/sidebar/","title":"浏览器插件ChatGPT Sidebar","section":"对话客户端类","content":" 浏览器插件 ChatGPT Sidebar ​配置教程 # 步骤 ​ # 官方链接:https://chatgpt-sidebar.com\n安装插件 修改设置, 将 URL 修改为 https://api.whatai.cc\n"},{"id":32,"href":"/docs/otherai/chat/cherrystudio/","title":"Cherry Studio","section":"对话客户端类","content":" Cherry Studio 配置教程 # Cherry Studio 项目简介 # Cherry Studio 是一款开源的多模型桌面客户端，旨在为用户提供一个统一、高效的AI服务平台。它兼容Windows、macOS 和Linux 三大操作系统，集成了包括OpenAI、DeepSeek、Gemini 等在内的多种主流大语言模型，并支持本地模型的运行。\nCherry Studio 高度自定义的设计、强大的扩展能力和友好的用户体验，使其成为专业用户和 AI 爱好者的理想选择。无论是零基础用户还是开发者，都能在 Cherry Studio 中找到适合自己的 AI 功能，提升工作效率和创造力。\n主要特点 # 多平台支持：支持 Windows、macOS 和 Linux。\n多模型支持：集成 OpenAI、DeepSeek、Gemini 等多种主流大语言模型，支持本地模型。\nAI助手与对话：内置超过300个预配置的AI助手，支持创建自定义AI助手，支持与多个AI模型同时对话。\n知识库管理：支持导入多种格式文件，构建个人AI知识库。\nAI绘画及其他集成工具：集成AI绘画、翻译、全局搜索、话题管理、WebDAV文件管理、Mermaid图表可视化等功能。\n自定义界面：支持主题切换、自定义CSS、会话布局、消息样式、头像设置等。\n集成外部服务：支持WebDAV、Notion、Tavily等服务集成。\n核心功能与特色 # 1. 基础对话功能 # 一问多答：支持同一问题通过多个模型同时生成回复，方便用户对比不同模型的表现，详见 对话界面。\n自动分组：每个助手的对话记录会自动分组管理，便于用户快速查找历史对话。\n对话导出：支持将完整对话或部分对话导出为多种格式（如 Markdown、Word 等），方便储存与分享。\n高度自定义参数：除了基础参数调整外，还支持用户填写自定义参数，满足个性化需求。\n助手市场：内置千余个行业专用助手，涵盖翻译、编程、写作等领域，同时支持用户自定义助手。\n多种格式渲染：支持 Markdown 渲染、公式渲染、HTML 实时预览等功能，提升内容展示效果。\n2. 多种特色功能集成 # AI 绘画：提供专用绘画面板，用户可通过自然语言描述生成高质量图像。\nAI 小程序：集成多种免费 Web 端 AI 工具，无需切换浏览器即可直接使用。\n翻译功能：支持专用翻译面板、对话翻译、提示词翻译等多种翻译场景。\n文件管理：对话、绘画和知识库中的文件统一分类管理，避免繁琐查找。\n全局搜索：支持快速定位历史记录和知识库内容，提升工作效率。\n3. 多服务商统一管理机制 # 服务商模型聚合：支持 OpenAI、Gemini、Anthropic、Azure 等主流服务商的模型统一调用。\n模型自动获取：一键获取完整模型列表，无需手动配置。\n多秘钥轮询：支持多个 API 秘钥轮换使用，避免速率限制问题。\n精准头像匹配：为每个模型自动匹配专属头像，提升辨识度。\n自定义服务商：支持符合 OpenAI、Gemini 、Anthropic 等规范的三方服务商接入，兼容性强。\n4. 高度自定义界面和布局 # 自定义 CSS：支持全局样式自定义，打造专属界面风格。\n自定义对话布局：支持列表或气泡样式布局，并可自定义消息样式（如代码片段样式）。\n自定义头像：支持为软件和助手设置个性化头像。\n自定义侧边栏菜单：用户可根据需求隐藏或排序侧边栏功能，优化使用体验。\n5. 本地知识库系统 # 多种格式支持：支持 PDF、DOCX、PPTX、XLSX、TXT、MD 等多种文件格式导入。\n多种数据源支持：支持本地文件、网址、站点地图甚至手动输入内容作为知识库源。\n知识库导出：支持将处理好的知识库导出并分享给他人使用。\n支持搜索检查：知识库导入后，用户可实时检索测试，查看处理结果和分段效果。\n6. 特色聚焦功能 # 快捷问答：在任何场景（如微信、浏览器）中呼出快捷助手，快速获取答案。\n快捷翻译：支持快速翻译其他场景中的词汇或文本。\n内容总结：对长文本内容进行快速总结，提升信息提取效率。\n解释说明：无需复杂提示词，一键解释说明不懂的问题。\n7. 数据保障 # 多种备份方案：支持本地备份、WebDAV 备份和定时备份，确保数据安全。\n数据安全：支持全本地场景使用，结合本地大模型，避免数据泄漏风险。\n配置方式 # 一、获取API秘钥（key） # 进入令牌页面（可点击下方链接直达），复制初始令牌或创建一个新的令牌\n令牌页面：👉 神马中转API工作台\n二、填入CherryStudio # 进入设置\u0026ndash;找到模型服务\u0026ndash;找到 NewAPI\n填入API秘钥（sk-xxx） 和 API地址（https://api.whatai.cc）\n打开右上角开关，找到NEW API，添加API秘钥和API地址：https://api.whatai.cc\n三、添加模型 # 选择并添加你所需要的模型，回到对话页面，选择添加的模型\nnano banana pro模型，填入模型ID：nano-banana-2-4k，端点类型选择图像生成（OpenAI）\n添加模型，模型值在神马中转API模型价格页面获取\n模型价格页面：👉 神马中转API模型价格\n回到对话页面选择已添加模型：\nCherry Studio界面UI # 使用 Cherry Studio（基础操作） # 下面讲一些日常使用和主要操作模块。\n4.1 对话与多模型切换 # 在主界面进入“对话 / 聊天”模块。\n在输入框上方通常可以选择当前使用的模型 / 服务商。你可以根据需求切换模型来比较答案。\n输入问题或提示，按回车发送，模型会生成回答。\n支持同时“多模型对比”输出（即同一个提示由多个模型同时生成答案）。 对话记录自动归类分组，方便查找历史对话。 可以将对话导出为 Markdown、Word 等格式。 4.2 智能体 / 助手管理 # 在 Cherry Studio 中，有“助手市场”或“智能体”机制，你可以直接使用预置助手或自己定义助手。\n内置助手：Cherry Studio 自带多个领域（写作、编程、翻译、知识问答等）的助手模块，直接选择即可使用。 自定义助手（智能体）：\n新建一个助手，给定名称、角色/指令、默认提示词等。\n可以设置该助手使用的模型、上下文策略、温度、最大令牌数等参数。\n通过助手入口或标签快速调起该助手。\n这样你可以为不同用途（如写作助手、客服助手、侧重代码助手等）分别定制多个助手。\n4.3 界面与布局调优 # 你可以在设置界面调整消息样式（气泡、列表）、消息字体、主题颜色等。 支持自定义 CSS：如果你懂 CSS，可通过自定义样式让界面更符合你喜好。 侧边栏模块（对话、知识库、绘图、设置等）可以隐藏或重排序，以精简界面。 全局搜索：可快速定位历史记录、知识库条目或对话内容。 4.4 绘画 # 使用Nano Banana Pro绘画\n添加智能体 / 助手 # 下面是更系统化地管理和使用智能体（助手）的步骤：\n创建助手模板\n在“助手 / 智能体”管理界面，点击“新建”或 “+ 助手”\n设置助手属性\n名称 / 标识：比如 “写作小助”、“代码君”\n角色指令 / 系统提示：预设一个角色指令或背景描述，引导模型以特定风格、用途回答\n默认提示词 / 模板：用户每次启动助手时带入一些固定提示\n参数设置：温度（随机性）、最大令牌、停止词、上下文长度等\n关联模型 / 服务商\n为该助手指定一个或多个优先使用的模型 / 服务商（若你配置了多个服务）\n快捷入口 / 标签\n给助手一个入口按钮或快捷调用方式，使你在对话界面能快速选择该助手\n管理 / 编辑 /复制\n后续可编辑助手配置、复制为新助手、删除不需要的助手\n通过这种机制，你能对话体验进行模块化与角色化控制，方便在不同任务场景（写作、客服、知识问答、编程）中切换使用。\n知识库教程（更进阶） # 知识库（Knowledge Base，简称 KB）是 Cherry Studio 的一个核心功能，用于将结构化 / 非结构化文档导入系统，通过向量检索辅助对话时回答更准确、更具上下文关联性。\n在 0.9.1 版本中，CherryStudio 带来了期待已久的知识库功能。\n下面我们将按步骤呈现 CherryStudio 的详细使用说明。\n添加嵌入模型 # 在模型管理服务中查找模型，可以点击“嵌入模型”快速筛选；\n找到需要的模型，添加到我的模型。\n创建知识库 # 知识库入口：在 CherryStudio 左侧工具栏，点击知识库图标，即可进入管理页面；\n添加知识库：点击添加，开始创建知识库；\n命名：输入知识库的名称并添加嵌入模型，以 bge-m3 为例，即可完成创建。\n添加文件并向量化 # 添加文件：点击添加文件的按钮，打开文件选择；\n选择文件：选择支持的文件格式，如 pdf，docx，pptx，xlsx，txt，md，mdx 等，并打开；\n向量化：系统会自动进行向量化处理，当显示完成时（绿色 ✓），代表向量化已完成。\n添加多种来源的数据 # CherryStudio 支持多种添加数据的方式：\n文件夹目录：可以添加整个文件夹目录，该目录下支持格式的文件会被自动向量化；\n网址链接：支持网址 url\n站点地图：支持 xml 格式的站点地图，如sitemap.xml；\n纯文本笔记：支持输入纯文本的自定义内容。\n提示：\n导入知识库的文档中的插图暂不支持转换为向量，需要手动转换为文本；\n使用网址作为知识库来源时不一定会成功，有些网站有比较严格的反扒机制（或需要登录、授权等），因此该方式不一定能获取到准确内容。创建完成后建议先搜索测试一下。\n一般网站都会提供sitemap，一般情况下在网站的根地址（即网址）后加/sitemap.xml可以获取到相关信息。如aaa.com/sitemap.xml 。\n如果网站没提供sitemap或者网址比较杂可自行组合一个sitemap的xml文件使用，文件暂时需要使用公网可直接访问的直链的方式填入，本地文件链接不会被识别。\n可以让AI生成sitemap文件或让AI写一个sitemap的HTML生成器工具；\n直链可以使用oss直链或者网盘直链等方式来生成。如果没有现成工具也可到ocoolAI官网，登录后使用网站顶栏的免费文件上传工具来生成直链。\n搜索知识库 # 当文件等资料向量化完成后，即可进行查询：\n点击页面下方的搜索知识库按钮；\n输入查询的内容；\n呈现搜索的结果；\n并显示该条结果的匹配分数。\n对话中引用知识库生成回复 # 创建一个新的话题，在对话工具栏中，点击知识库，会展开已经创建的知识库列表，选择需要引用的知识库；\n输入并发送问题，模型即返回通过检索结果生成的答案 ；\n同时，引用的数据来源会附在答案下方，可快捷查看源文件。\n使用示例（多个场景演示） # 下面给出几个典型的使用示例，以帮助你理解如何在实际场景中应用 Cherry Studio 的功能组合。\n示例 1：学术问答 + 引用知识库 # 场景：你有一篇关于“量子力学基础”的课程讲义（PDF 文件），希望导入后让 AI 在对话中直接引用讲义内容回答问题。\n步骤：\n在 “知识库” 模块中新建一个知识库，命名为 “量子力学讲义”，选择 embedding 模型如 bge-m3。\n上传该讲义的 PDF 文件，Cherry Studio 自动将其拆分并向量化。\n在知识库界面测试搜索，比如 “什么是叠加态？” 搜索结果应显示对应片段。\n返回对话界面，开启知识库模式，选择 “量子力学讲义” 作为参考知识库。\n提问：“解释一下量子叠加态原理”，模型在检索相关知识片段基础上生成回答，并在末尾给出片段来源。\n若需要继续追问：“那如何理解干涉实验中的叠加？” AI 会基于知识库上下文继续回答。\n效果是：AI 的回答更具专业性，也能快速定位源文档中的原段落。\n示例 2：写作助手 + 多模型对比 # 场景：你正在写一篇产品文案，希望获得多个版本的草稿，以便进行比较。\n步骤：\n在对话界面中选择“多模型对比”模式（若有此功能）。\n输入提示词，比如 “为某款智能音箱写一段广告文案，突出音质与便携性”。\n系统同时向多个模型（如 DeepSeek、OpenAI GPT、Gemini）发起请求，返回多个版本文案。\n你可以对比各版本优缺点（风格、措辞、语气）并选择喜欢的版本进一步润色。\n若需要，可以启动一个定制的“文案助手”助手，让它专注润色/校正语法风格。\n这种方式可以快速跳出思路瓶颈，获得多种创意备选。\n示例 3：图像生成 + 文本说明 # 场景：你在做一份营销海报，需要 AI 帮你生成一张主题插图，并写一句配文。\n步骤：\n切换到“绘图 / 图像”模块，输入提示词：\n“极简风格，现代感科技音箱产品，夜景背景，柔和灯光，3D 效果”\n点击生成，等待模型返回图像。\n下载或保存图片。\n切换回对话界面，对 AI（或使用写作助手）输入提示：\n“请为刚才那张插图写一句简洁有力的宣传语，适合海报使用。”\nAI 给出一句适配图片风格的文案，例如 “音质 · 释放自由” 或 “小体积，大声音”\n如不满意，可以继续调优提示（图像或文案）进行多轮迭代。\n最终你将获得一张贴合主题风格的图像 + 一句配文，方便直接用于设计或海报制作。\n示例 4：客服问答 / 智能体部署 # 场景：你有一份产品常见问题文档（FAQ），希望部署一个客服助手，让用户输入问题时自动返回标准答案。\n步骤：\n新建一个“FAQ 知识库”，上传 FAQ 文档（Word、PDF 或 TXT 格式）。\n向量化后测试检索。\n新建一个“客服助手”智能体，设置其角色提示词为 “你是产品客服助手，请根据知识库提供精准答案，不要无中生有”。\n将该助手关联到“FAQ 知识库”。\n用户在对话中输入问题，如 “如何退换货？”\n系统首先检索知识库，得到对应段落作为上下文，再由助手生成答案并附上引用源。\n若知识库内无匹配且业务允许，助手可以 fallback 到基础大模型生成回答或提示无人知晓。\n这样，你便可以快速搭建一个具有知识库支撑的客服问答系统。\n"},{"id":33,"href":"/docs/errorcode/","title":"常见错误以及解决办法","section":"神马中转API 一站式AI大模型API聚合平台 · 行业领先","content":" 常见错误以及解决办法 ​ # Q：切换到了 GPT-4，询问它是不是 GPT-4，为什么回答不是？ ​ # A： 首先，如果你问 GPT-4：“你是不是 GPT-4？”它大概率会回答：“我是 OpenAI 的 GPT-3 模型，目前还没有 GPT-4。”之所以会这样，是因为 OpenAI 开放给 API 调用的 GPT-4，训练数据都是 2021 年 9 月之前的。模型训练好之后，如果不重新训练，并不会自动更新里面的知识，这就好像我问 2021 年的你 2023 年第一顿饭你吃了什么，答案一定是错的。\nQ：为什么 ChatGPT Plus 的 GPT-4 能回答出自己是 GPT-4？ ​ # A： 简单来说，ChatGPT Plus 使用的模型版本和开放给 API 的并不一样，作为内部版本，很大可能会用更新的数据去训练，甚至是实时数据训练。虽然都叫 GPT-4，但给出的答案不同，因为训练数据不同。\nQ：那我如何去判断他是否是 GPT-4 模型？ ​ # A： 可使用以下逻辑性问题进行测试。 问题： 鲁迅和周树人是什么关系？ GPT-3.5： 鲁迅和周树人是两个不同的人 GPT-4： 鲁迅和周树人是同一个人。\nQ：无法登录？ ​ # A： 请确保用户名填写正确，不要填写邮箱地址，填写你注册时的用户名。如遇到登录问题无法自行解决，请联系客服，第一时间为您处理。\nQ：为什么请求后没吐字没补全 token？ ​ # A： 有以下可能：\n快吐字了，客户端断开连接。\ntools call 或 function call。\nOpenAI 直接返回 [Done]，一般是政策安全相关拒绝回答，需要结合返回的 finish_reason 或内容进行判断。\nA： 针对不同的数据返回代码，以下是常见的错误代码：\n错误代码 代码解释 400 Bad Request 请求格式错误或无效。这通常意味着你的请求参数有误，需要你检查并修正请求参数。 401 Unauthorized 请求令牌无效。这通常意味着你的请求令牌有误，需要你检查并修正请求参数。 403 Forbidden 一般是余额不足。 404 Not Found 请求的资源未找到。你可能正在试图访问一个不存在的端点。 413 Request Entity Too Large 请求体太大。你可能需要减少你的请求数据量。 429 Too Many Requests 由于短时间内发送过多的请求，你已经超过了你的速率限制。 500 Internal Server Error 服务器内部错误。这可能是 OpenAI 服务器的问题，不是你的问题。 503 Service Unavailable 服务暂时不可用。这可能是由于 OpenAI 正在进行维护或者服务器过载。 Q：后台额度充足，使用 API 提示额度不足？ ​ # A： 请确认你后台创建的令牌已经分配好额度，过期时间一般可以设置成无限制。另外，额度不是填写金额，500000 额度 = 1 美金，可按需填写。\nQ：出现 CDN 回源报错？ ​ # A： 请联系客服获取企业客户接口地址。\nQ：提示当前分组下没有可用渠道？ ​ # A： 请确保模型名称完全和列表一致，并且区分大小写。\nQ：额度是什么? 怎么计算的？ ​ # A： 额度 = 分组倍率 * 模型倍率 * (提示 token + 补全 token * 补全倍率)。 其中补全倍率对于 GPT-3.5 固定为 1.33，GPT-4 为 2，与官方保持一致。如果是非流模式，官方接口会返回消耗的总 token，但是你要注意提示和补全的消耗倍率不一样。\nQ：什么是上下文？ ​ # A： 在 GPT 用于文本生成时，它需要考虑之前输入的所有文本上下文，以生成连贯、有意义的句子。随着输入上下文的增加，GPT 生成的文本变得越来越连贯和精准。例如，如果将一篇完整的文本或段落作为输入，GPT 将能生成符合上下文连贯性的自然语言文本。因此，GPT 上下文累积得越多，生成文本的准确度和连贯性呈逐步提升趋势。\nQ：账户额度足够为什么提示额度不足？ ​ # A： 请检查你的令牌额度是否足够，这个和账户额度是分开的。令牌额度仅供用户设置最大使用量，用户可自由设置。\nQ：ChatGPT Next Web 报错: Failed to fetch？ ​ # A： 部署的时候不要设置 BASE URL。检查你的接口地址和 API Key 有没有填对。\nQ：网站部分页面打开报错？ ​ # A： 请清理浏览器缓存和 Cookie。\nQ：为什么 gpt-4 额度消耗这么快？ ​ # A： gpt-4 的消耗速度是 gpt-3.5-turbo 的 20 到 40 倍。假设购买了 9w token，我们用 30 倍作为平均倍率，也就是 90000 / 30 = 3000 字左右，加上每次要附带上历史消息，能发的消息数将会进一步减半。在最极限的情况下，一条消息就能把 9w token 消耗完，所以请谨慎使用。\nQ：令牌无效？ ​ # A： 一般出现这种问题，可能是：\n地址写错，不是我们的地址。\n令牌没有正确的设置到程序中。\n令牌已失效，请重新生成一个新的令牌。\nQ：Failed to fetch？ ​ # A： 一般遇到这种问题，可能是你的网络环境有问题，请尝试更换网络。亦或者是你使用了错误的接口地址，请检查接口地址是否正确。\nQ：无可用渠道？ ​ # A： 检查错误中得模型名称是否存在。\nQ：构图时出现 SyntaxError：Unexpected token“\u0026lt;” …… ​ # A： 出现这个问题仍是使用了不正确的接口，需要参考上面的第三个问题换成正确接口即可解决问题。\nQ：使用 chatgpt-web-midjourney-proxy 上传失败？ ​ # A： 对于这个问题，作者已经在项目地址反复说明了很多遍： 。需要使用 docker 部署，并开启 API_UPLOADER，vercel 不支持。而且如果是在前端填写的中转地址，上传文件会跟着中转地址走。\nQ：该令牌额度已用尽？ ​ # A： 这个问题一般说明你的令牌已经没有额度了，需要给你的令牌加额度，或者你的钱包已经没有额度了，需要给你的钱包充值。\nQ：user quota is not enough？ ​ # A： 这个问题一般说明你的账户已经没有额度了，需要给你的钱包充值。\nQ：max_tokens is too large？ ​ # A： 请求参数中的 max_tokens 参数设置过大。请求携带的上下文 token 数 + max_tokens 必须小于等于模型的上下文大小。例如，gpt-4 的模型上下文是 128k，max_tokens 最大为 4096。那么必须满足 max_tokens \u0026lt;= 4096 和 请求携带的上下文 token 数 + max_tokens \u0026lt;= 128k 。\n"},{"id":34,"href":"/docs/ailearn/notice/","title":"高级功能-通知设置 · 额度预警通知","section":"站内基础教程","content":" 高级功能-通知设置 · 额度预警通知 # 🔔 事件订阅与通知配置指南 # 关键事件，实时掌控！\n订阅重要事件并设置通知渠道，额度预警、系统更新等消息即时送达，让您永远快人一步 🚨\n💻 配置入口：\n👉神马中转API个人中心\n三步配置法（以额度预警为例） # 1️⃣ 选择订阅事件\n🔍 可订阅事件清单 # 事件类型 触发条件 频率限制 推荐场景 公告通知 平台发布重要公告 无限制（实时推送） 所有用户必选 ✅ 营销信息 平台推出优惠活动/新产品 无限制（实时推送） 关注优惠活动的用户 用户额度预警 总钱包余额 \u0026lt; 您设置的阈值 ≤1条/小时（防骚扰） 所有用户必选 ✅ Key额度预警 单个API Key余额 \u0026lt; 您设置的阈值 Key独立 ≤1条/小时 管理多Key的企业用户 2️⃣ 配置通知渠道\n🔧 通知渠道配置详解 # 渠道 配置难度 适用场景 关键配置项 实时性 邮件 ⭐☆☆☆☆ 非紧急通知/存档记录 邮箱地址（自动识别登录邮箱） 5分钟内 企业微信 ⭐⭐☆☆☆ 团队协作/内部告警 企业ID + 应用Secret + AgentID 即时 钉钉 ⭐⭐☆☆☆ 企业级监控/运维警报 Webhook URL + 密钥 即时 WxPusher ⭐☆☆☆☆ 个人微信接收通知 UID（扫码绑定） 即时 Webhook ⭐⭐⭐⭐☆ 系统集成/自动化流程 API端点URL + 自定义Header/Body模板 即时 Telegram ⭐⭐☆☆☆ 国际用户/开发者 Bot Token + Chat ID 即时 3️⃣ 点击保存生效\n遇到问题？随时召唤客服！\n"},{"id":35,"href":"/docs/aidocs/pay2/","title":"基本概念-按次付费","section":"基础知识","content":" 📖基本概念-按次付费 # 💸 按次计费：像买奶茶一样简单！ # 不问字数多少，一次对话一票到底！ 就像坐地铁——无论坐1站还是10站，票价都一样\n对话场景 字数统计 按次计费 按量计费对比 简单问候 问5字 + 答15字 ✅$0.1 ❌ $0.0012 问：\u0026ldquo;你好\u0026rdquo; (固定消费) (输入$0.00015) 答：\u0026ldquo;请问需要帮助吗？\u0026rdquo; (输出$0.0009) 深度咨询 问200字+答800字 ✅$0.1 ❌ $0.054 问：\u0026ldquo;请分析2023全球\u0026hellip;\u0026rdquo; (固定消费) (输入$0.006) 答：\u0026ldquo;根据IMF报告\u0026hellip;\u0026rdquo; (输出$0.048) 💎 黄金规律：当字数 \u0026gt;1000字时，按次计费更划算！\n适用场景导航 # bash复制代码# 👍 强烈推荐使用 ➜ [ 写长文 ] [ 代码调试 ] [ 报告分析 ] [ 法律咨询 ] [ 学术修改 ] [ 剧本创作 ] # ️ 不推荐使用 ➜ [ 查天气 ] [ 简单翻译 ] [ 短问答 ] "},{"id":36,"href":"/docs/openai/wltpjx/","title":"网络图片解析","section":"Openai请求格式（通用）","content":" 网络图片解析 # 接口说明 # 通过多模态AI模型分析图片内容，理解图片、提取图片信息，包括OCR功能。\n主流图片分析模型 # 模型名称 描述 gpt-4o 目前图片分析调用量最大的模型，稳定、并发高 gemini-2.5-flash 谷歌旗舰模型，速度快，性价比好 claude-sonnet-4-20250514 图片分析做的不错，但性价比略差 doubao-1.5-vision-pro-250328 国内图片分析主流模型，性价比好，稳定、并发高 基础信息 # Base URL: https://api.whatai.cc\n请求方式: POST\nContent-Type: application/json\n接口地址 # POST /v1/chat/completions\n请求头 # 参数 类型 必填 说明 Authorization string 是 Bearer token，格式：Bearer ****** Content-Type string 是 固定值：application/json User-Agent string 否 客户端标识 请求参数 # { \u0026#34;model\u0026#34;: \u0026#34;gemini-2.0-flash-thinking-exp-1219\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: [ {\u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;你是一个图片分析助手。\u0026#34;} ] }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;image_url\u0026#34;, \u0026#34;image_url\u0026#34;: { \u0026#34;url\u0026#34;: \u0026#34;图片URL\u0026#34; } }, { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;分析提示词\u0026#34; } ] } ], \u0026#34;temperature\u0026#34;: 0.1, \u0026#34;user\u0026#34;: \u0026#34;whatai\u0026#34; } 参数说明 # 参数 类型 必填 说明 model string 是 模型名称，推荐：gemini-2.5-flash 或 gpt-4o messages array 是 消息内容数组 temperature float 否 生成文本的随机性，0-1之间 user string 否 用户标识 Python 调用示例 # import requests # API配置 BASE_URL = \u0026#34;https://api.whatai.cc/\u0026#34; API_ENDPOINT = BASE_URL + \u0026#34;v1/chat/completions\u0026#34; API_KEY = \u0026#34;sk-******\u0026#34; # 替换为你的API密钥 IMAGE_URL = \u0026#34;https://api.whatai.cc/111.jpg\u0026#34; # 替换为你的图片URL def analyze_image(image_url, prompt): \u0026#34;\u0026#34;\u0026#34; 图片分析函数 :param image_url: 图片URL :param prompt: 分析提示词 :return: 分析结果文本 \u0026#34;\u0026#34;\u0026#34; payload = { \u0026#34;model\u0026#34;: \u0026#34;gemini-2.0-flash-thinking-exp-1219\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: [{\u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;你是一个图片分析助手。\u0026#34;}] }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: [ {\u0026#34;type\u0026#34;: \u0026#34;image_url\u0026#34;, \u0026#34;image_url\u0026#34;: {\u0026#34;url\u0026#34;: image_url}}, {\u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: prompt} ] } ], \u0026#34;temperature\u0026#34;: 0.1 } headers = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Authorization\u0026#34;: f\u0026#34;Bearer {API_KEY}\u0026#34; } try: response = requests.post(API_ENDPOINT, headers=headers, json=payload) response.raise_for_status() return response.json()[\u0026#34;choices\u0026#34;][0][\u0026#34;message\u0026#34;][\u0026#34;content\u0026#34;] except Exception as e: print(f\u0026#34;请求失败: {e}\u0026#34;) return None # 使用示例 if __name__ == \u0026#34;__main__\u0026#34;: result = analyze_image(IMAGE_URL, \u0026#34;请描述这张图片的内容\u0026#34;) if result: print(\u0026#34;分析结果:\u0026#34;, result) 常见错误码 # 状态码 说明 400 请求参数错误，看看图片是不是太大了 500 服务器内部错误 "},{"id":37,"href":"/docs/otherai/chat/chatnextweb/","title":"Chat Next Web","section":"对话客户端类","content":" Chat Next Web # "},{"id":38,"href":"/docs/modellearn/claudeapi/","title":"Claude API格式兼容","section":"模型接入教程","content":" 神马中转API实现任意模型Claude Compatible接入！ # 随着 Claude Code 在开发者中的普及，越来越多的用户希望能在只支持 Claude 接口的应用中调用更多模型。神马中转API 率先实现了“任意模型 Claude Compatible”，极大推动了 AI 生态的开放和互联！\n🧠 Claude 格式 vs OpenAI 格式对比 # 🎯 请求路径区别 # 接口类型 示例请求路径 OpenAI 格式 api.whatai.cc/v1/chat/completions Claude 格式 api.whatai.cc/v1/messages 🧾 请求体格式差异 # OpenAI 格式请求示例：\n{ \u0026#34;model\u0026#34;: \u0026#34;gpt-4.1\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Hello!\u0026#34; } ] } Claude 格式请求示例：\n{ \u0026#34;model\u0026#34;: \u0026#34;claude-3-5-sonnet-20240620\u0026#34;, \u0026#34;max_tokens\u0026#34;: 1024, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Hello, world\u0026#34; } ] } 关键差异：\n参数结构：Claude 请求中必须显式指定 max_tokens；OpenAI 中为可选。\n字段命名：如 messages 等字段虽然类似，但默认行为及语义略有差异。\n📦 响应格式差异对比 # OpenAI 返回示例 # { \u0026#34;choices\u0026#34;: [ { \u0026#34;finish_reason\u0026#34;: \u0026#34;stop\u0026#34;, \u0026#34;index\u0026#34;: 0, \u0026#34;logprobs\u0026#34;: null, \u0026#34;message\u0026#34;: { \u0026#34;annotations\u0026#34;: [], \u0026#34;content\u0026#34;: \u0026#34;Hello! How can I assist you today? 😊\u0026#34;, \u0026#34;refusal\u0026#34;: null, \u0026#34;role\u0026#34;: \u0026#34;assistant\u0026#34; } } ], \u0026#34;created\u0026#34;: 1753384149, \u0026#34;id\u0026#34;: \u0026#34;chatcmpl-BwvbRo5aJ8TXqFwmmOXsE7eicJig3\u0026#34;, \u0026#34;model\u0026#34;: \u0026#34;gpt-4.1\u0026#34;, \u0026#34;object\u0026#34;: \u0026#34;chat.completion\u0026#34;, \u0026#34;system_fingerprint\u0026#34;: \u0026#34;fp_07e970ab25\u0026#34; } Claude 返回示例 # { \u0026#34;id\u0026#34;: \u0026#34;msg_01NufbyqybJo5fVtRkH1VPzk\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;message\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;assistant\u0026#34;, \u0026#34;model\u0026#34;: \u0026#34;claude-3-5-sonnet-20240620\u0026#34;, \u0026#34;content\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Hello! How can I assist you today? Is there anything specific you\u0026#39;d like to talk about or any questions you have?\u0026#34; } ], \u0026#34;stop_reason\u0026#34;: \u0026#34;end_turn\u0026#34; } 主要结构差异：\n项目 OpenAI Claude 回复结构 使用 choices 数组 顶层 content 数组 内容位置 choices[0].message.content content[0].text 元信息 包括 created, id, model 等 命名结构不同但信息类似 停止标识 finish_reason stop_reason 🚧 接口不兼容性的挑战 # 虽然两者都使用 JSON 结构，但在：\n流式响应\n函数调用\n系统提示词的结构\n返回格式嵌套层级\n等方面存在较大差异，导致 OpenAI 与 Claude 的 API 完全不兼容。\n🚀 神马中转API：让任意模型秒变 Claude Compatible！ # 为解决这一痛点，神马中转API 推出了革命性的 API 中转兼容方案：\n✅ 兼容 Claude Code 接口标准\n✅ 支持平台自营全部模型\n✅ 支持接入自定义第三方模型\n✅ 统一封装为 Claude 格式，自动协议转换\n🌐 一站式模型整合平台 # 开发者可将任何模型（无论底层是否兼容 Claude 格式）通过神马中转API 的“自定义 API 功能”接入平台。神马中转API将自动完成协议适配，统一转化为 Claude Compatible 接口，无需手动修改代码。\n🌈 总结：连接模型生态的桥梁 # 神马中转API 打破了模型协议的壁垒，让所有模型都能：\n被 Claude Code 等工具无缝调用\n灵活集成进任何只支持 Claude 的应用\n降低开发和迁移成本，提升跨平台效率\n未来 AI 世界的“多模型互联”将成为常态，而神马中转API 正在为此铺平道路。\n如果你希望在 Claude Code 或其他仅支持 Claude 接口的环境中使用更多模型，现在就是最佳时机。\n🧩 即刻开启 Claude Compatible 模型整合之旅，让所有模型为你所用！\n"},{"id":39,"href":"/docs/modellearn/midjourney/","title":"Midjourney使用教程","section":"模型接入教程","content":" Midjourney使用教程 ​ # Midjourney（简称mj）和Niji-journey（简称niji）模式相同，都以mj举例\n✨ 支持Midjourney官方所有功能\n同时支持, Midjourney Proxy Plus, 以及, Midjourney Proxy 接口协议\n如果你的项目不支持以上方式，请, 查看API接入文档, ，实现调用接口\n适用性广，支持 Midjourney 所有操作\n一、MJ Proxy Plus 快捷接入方式 ​ # Midjourney Proxy 主机： https://api.whatai.cc\nMidjourney Proxy Secret：在令牌页生成的令牌，格式为 sk-xxxxxx\nChatGPT \u0026amp; Midjourney配置教程： # 神马中转API聊天菜单 - ChatGPT \u0026amp; Midjourney - 左下角设置按钮 - 服务端 - 找到Midjourney设置：\n神马中转API聊天菜单 - ChatGPT \u0026amp; Midjourney - 绘画：\n提示词\nOn the streets of the city after the rain, the second-dimensional beauty wears a silver-gray hip skirt and gray stockings with patterns. Her high heels are eye-catching, in gray and deep purple tones. She has an urban fashion style. The stockings have exquisite patterns. The street lights and rainwater reflect the elegant legs. 不同的客户端需要填写不同的 BASE_URL ，请尝试以下地址： ​ # https://api.whatai.cc\nhttps://api.whatai.cc/mj\nhttps://api.whatai.cc/mj/\nhttps://api.whatai.cc/mj/submit/imagine\nmode 参数（可选）： ​ # relax, ：慢速（1-10分钟出图）\nfast, ：快速（默认）（1-5分钟出图）\nturbo, ：极速（1分钟内出图）\nproxy-type 参数（可选）： ​ # origin, ：图片使用 Discord 原地址，国外访问快，国内可能无法访问\nrelay, ：图片使用服务转发地址，全球访问快（默认）\nproxy, ：图片使用管理员设置的地址，国内访问快（令牌可设置自己的图片代理地址）\n二、切换绘图模式、图片代理 # 设置参数介绍： # fast, 是 Midjourney 的 fast mode（快速绘图模式），通常 1-5分钟内完成绘图\nturbo, 是 Midjourney 的 turbo mode（极速绘图模式），通常 1分钟内完成绘图\nrelax, 是 Midjourney 的 relax mode（慢速绘图模式），通常 1-20分钟内完成绘图\nrelay, 是使用中转站地址转发，图片国内访问较快\norigin, 是使用 Discord 原地址，图片国外访问快，国内不可访问\nproxy, 是使用图片代理地址，替换, https://cdn.discordapp.com, 的地址，图片国内访问较快\n设置优先级\n令牌 \u0026gt; 路径参数 \u0026gt; 系统默认\n1️⃣ 通过令牌管理控制（推荐） ​ # 在 ，选择要使用的令牌，点击编辑，设置想要管理的设置。\n切换绘图模式方式，调整绘图速度 设置返回图片地址，方便国内访问（可使用自己的图片代理）\n在令牌编辑页设置自己的图片代理地址\nPS: →, 自建图片代理教程, ，需要自备海外服务器，最好是国内线路优化、带宽大的\n2️⃣ 通过URL路径参数 ​ # 切换绘图模式方式，调整绘图速度, /mj 默认是 fast mode/mj-fast/mj 是 fast mode/mj-turbo/mj 是 turbo mode/mj-relax/mj 是 relax mode示例：https://api.whatai.cc/mj-turbo/mj/submit/imagine 填写到程序中时，一般为：https://api.whatai.cc/mj-turbo 切换返回图片地址格式, /mj 是管理员设置的默认方式/mj-relay/mj 使用服务转发地址，图片国内访问较快/mj-origin/mj 使用 Discord 原地址，图片国外访问快/mj-proxy/mj 使用管理员设置的代理地址，图片国内访问较快示例：https://api.whatai.cc/mj-relay/mj/submit/imagine 填写到程序中时，一般为：https://api.whatai.cc/mj-relay 同时支持多个参数\n格式为：/mj-{mode}-{image_proxy}\n示例： https://api.whatai.cc/mj-turbo-origin，采用【极速模式】，返回【Discord 原地址】。https://api.whatai.cc/mj-relax，采用【慢速模式】，返回【服务转发地址（默认）】。https://api.whatai.cc，采用【快速模式（默认）】，返回【服务转发地址（默认）】。\n三、New API/Chat API/Shell API 接入MJ ​ # 渠道类型无所谓，只需渠道模型里有 Midjourney\n配置代理地址为：https://api.whatai.cc\n填写密钥为令牌页生成的令牌\n四、Midjourney 价格表 ​ # 计费价格请参考, 模型价格页面的Midjourney栏 ：https://api.whatai.cc/models?provider=Mid-journey\n类型 1 端点 ：价格随绘图模式（Fast/Relax/Turbo）变化\n类型 2 端点 ：价格固定不变\n类型 3 端点：免费端点，主要用于查询、继续完成任务\n类型 1 端点包括： ​ # 属性 解释 Outpaint 变焦 Pan 焦点移动 Blend 混图 Inpaint 局部重绘 Upscale 2x 放大2倍 Upscale 4x 放大4倍 PicReader 图生文后生成图片 Reroll 重新生成 类型 2 端点包括： ​ # 属性 解释 Upscale 放大 Describe 图生文 PicReaderRetry 图生文重新生成 FaceSwap 换脸 Shorten Prompt 分析 类型 3 端点包括： ​ # 属性 解释 Fetch 获取单个任务进度 ListByIDs 批量获取任务 Modal 确认提交弹窗任务 Seed 获取 Seed "},{"id":40,"href":"/docs/openai/bdtpjx/","title":"本地图片解析","section":"Openai请求格式（通用）","content":" 本地图片解析 # 接口说明 # 通过多模态AI模型分析图片内容，支持OCR和图片信息提取功能。\n主流图片分析模型\n模型名称 描述 gpt-4o 目前图片分析调用量最大的模型，稳定、并发高 gemini-2.5-flash 谷歌旗舰模型，速度快，性价比好 claude-sonnet-4-20250514 图片分析做的不错，但性价比略差 doubao-1.5-vision-pro-250328 国内图片分析主流模型，性价比好，稳定、并发高 本地图片分析预处理 # 需要先把本地图片转为 base64 再提交给模型。\n参数说明 # model: 指定使用的AI模型\nmessages: 包含用户指令和图片数据\ntemperature: 控制输出随机性(0-1)\nimage_url: 支持Base64编码的本地图片或网络图片URL\n请求示例 # import base64 import requests def encode_image(image_path): \u0026#34;\u0026#34;\u0026#34;将本地图片编码为Base64字符串\u0026#34;\u0026#34;\u0026#34; with open(image_path, \u0026#34;rb\u0026#34;) as image_file: return base64.b64encode(image_file.read()).decode(\u0026#34;utf-8\u0026#34;) # API配置 BASE_URL = \u0026#34;https://api.whatai.cc/\u0026#34; API_ENDPOINT = BASE_URL + \u0026#34;v1/chat/completions\u0026#34; API_KEY = \u0026#34;sk-******\u0026#34; # 替换为你的API密钥 # 准备请求数据 image_data = encode_image(\u0026#34;example.png\u0026#34;) # 本地图片路径 payload = { \u0026#34;model\u0026#34;: \u0026#34;gpt-4o\u0026#34;, # 指定分析模型 \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: [ {\u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;请分析图片内容\u0026#34;}, { \u0026#34;type\u0026#34;: \u0026#34;image_url\u0026#34;, \u0026#34;image_url\u0026#34;: { \u0026#34;url\u0026#34;: f\u0026#34;data:image/png;base64,{image_data}\u0026#34; } } ] } ], \u0026#34;temperature\u0026#34;: 0.1 } headers = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Authorization\u0026#34;: f\u0026#34;Bearer {API_KEY}\u0026#34; } # 发送请求 response = requests.post(API_ENDPOINT, headers=headers, json=payload) print(response.json()) # 输出响应结果 注意事项 # 图片需小于20MB\n支持PNG/JPEG格式；非普通扩展名的图片（例如.file）请先处理成base64再给模型。\n响应时间取决于图片大小和模型选择\n"},{"id":41,"href":"/docs/ailearn/keysafety/","title":"高级功能-API KEY安全配置","section":"站内基础教程","content":" 高级功能-API KEY 安全配置 # 🔐 API Key 安全加固指南 # 密钥如金，严防盗刷！ 三大利刃全面守护您的API Key安全，杜绝泄露风险\n🛡️ 安全配置三剑客 # ⚔️ 1. IP白名单 - 网络级防护 # 作用：构建数字围墙，只有在白名单内的 IP 地址才能调用 API，其他 IP 地址的请求将被拒绝。\n令牌页 → 目标Key → 编辑 → IP白名单 ⚠️ 致命防护：\n未在白名单的IP发起请求 → 403 Forbidden\n有效拦截99%的盗刷攻击\n2. 额度预警 - 财务级护航 # 作用：实时监控消耗，预防超额损失\n个人中心页 → 通知设置 → 额度预警订阅 🔒 3. 模型锁 - 权限级管控 -启用模型限制 # 作用：指定某个 API Key 仅用于特定的模型，防止该 Key 被用于其他模型调用\n启用效果：\n允许： gpt-4-turbo- 拒绝： midjourney-v6（返回403错误） 令牌页 → 目标Key → 编辑 → 模型限制 🛠️ 配置实战演示 # 案例：为客服系统Key配置安全策略\nStep 1：进入编辑页 Key名称：AI客服生产环境 Step 2：设置IP白名单 添加 → 122.112.10.25（客服服务器IP） 添加 → 122.112.10.0/24（备用网段） Step 3：启用模型锁 勾选模型 → gpt-4-turbo-2025 勾选模型 → whisper-asr Step 4：绑定额度预警 预警阈值 → $50 通知渠道 → 企业微信+短信 🌐 攻击防护模拟测试 # # 攻击者视角（密钥泄露后）： 1. 从境外IP 58.100.2.3 发起请求 → ❌ 被IP白名单拦截 2. 伪造境内IP尝试 → ❌ 非授信IP段仍被拒 3. 尝试调用mj模型 → ❌ \u0026#34;该Key未授权此模型\u0026#34; 4. 小额盗刷测试 → ✅ 触发预警短信 → 管理员立即冻结 ⚠️ 必须规避的配置误区 # - 错误1：IP白名单留空（等于全开放） + 修正：至少添加1个IP段 - 错误2：预警阈值=$0.1（频繁干扰） + 修正：设为日均消耗的 "},{"id":42,"href":"/docs/otherai/chat/GoAmz/","title":"GoAmz","section":"对话客户端类","content":" GoAmz # "},{"id":43,"href":"/docs/ailearn/errorcode/","title":"常见报错码","section":"站内基础教程","content":" 常见报错码 # 错误排查指南 # 遇到错误时，请先查看返回的错误码，然后参考下表进行排查处理。\n错误码对照表 # 状态码 说明 解决方案 400 请求格式错误 检查请求参数，例如o1系列模型不支持system参数 401 无效令牌 检查API密钥是否正确，可换模型测试验证 403 令牌分组被禁用 编辑令牌取消限额或新建令牌 404 接口不存在 检查Base URL是否正确，尝试添加/v1或斜杠/ 413 请求内容过长 缩短prompt内容后重试 429 上游限流 账号并发过高，稍后重试 500 服务器内部错误 多次重试仍失败请联系管理员 503 模型不可用 当前分组无该模型渠道，请检查模型名称是否正确，有没有多空格少空格之类。 504 网关超时 上游服务器未及时响应，稍后重试 524 连接超时 通道拥挤，稍后重试 解答 # 400 错误码，一般是请求参数不匹配。先把 system 注释掉，先试试通不通。\n401 错误码 “无效令牌”，是因为令牌和API地址URL错配了。\n403 错误码，一般是 令牌额度 不够了（令牌额度和账户额度是两个概念）。 到令牌编辑里，点 “设为无限额度” 就可以了，一般等2分钟就能生效。 也可以新建一个令牌“设为无限额度”，试一下。\n404 错误码，大概率是 API地址 没正确设置。\n429 错误码，是模型达到流量限制的体现。简单说就是：用的人太多 造成模型达到 TPM 饱和了。 一般情况下，等会重试就可以使用。 如果持续不能使用，把模型名称给客服，客服会排查处理。\n503 错误码，请检查模型全称有没有错，模型全称到 顶部菜单 支持模型 页面查看。 如果模型名称没错，把模型名称发给客服，客服进行排查处理。\n"},{"id":44,"href":"/docs/openai/functioncall/","title":"函数调用FunctionCall","section":"Openai请求格式（通用）","content":" 函数调用FunctionCall # 概念介绍 # 函数调用(Function Calling\\Tools Calling)是AI大模型的一种能力。允许大语言模型在对话过程中调用外部函数/工具。当用户提问需要实时数据(如天气、股票等)时，模型会返回函数调用请求，开发者可以在后端执行相应函数并返回结果。\nAPI 基础信息 # 请求地址: https://api.whatai.cc/v1/chat/completions\n请求方法: POST\n认证方式: Bearer Token\n请求示例 # import http.client import json # 创建HTTPS连接 conn = http.client.HTTPSConnection(\u0026#34;api.whatai.cc\u0026#34;) # 构造请求体 payload = json.dumps({ \u0026#34;model\u0026#34;: \u0026#34;gpt-4o\u0026#34;, # 指定模型 \u0026#34;max_tokens\u0026#34;: 300, # 最大返回token数 \u0026#34;temperature\u0026#34;: 0.8, # 生成结果的随机性控制 \u0026#34;stream\u0026#34;: False, # 是否流式输出 \u0026#34;messages\u0026#34;: [{ \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;上海今天几度？\u0026#34; # 用户提问 }], \u0026#34;tools\u0026#34;: [{ # 定义可用工具 \u0026#34;type\u0026#34;: \u0026#34;function\u0026#34;, \u0026#34;function\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;get_current_weather\u0026#34;, # 函数名称 \u0026#34;description\u0026#34;: \u0026#34;获得天气信息\u0026#34;, # 功能描述 \u0026#34;parameters\u0026#34;: { # 参数定义 \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;location\u0026#34;: { # 必填参数：地点 \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;城市和州名，例如：上海, 中国\u0026#34; }, \u0026#34;unit\u0026#34;: { # 可选参数：温度单位 \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;enum\u0026#34;: [\u0026#34;celsius\u0026#34;, \u0026#34;fahrenheit\u0026#34;] } }, \u0026#34;required\u0026#34;: [\u0026#34;location\u0026#34;] # 必填参数列表 } } }] }) # 请求头设置 headers = { \u0026#34;Accept\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Authorization\u0026#34;: \u0026#34;Bearer sk-**********************\u0026#34;, # 替换为你的 API 令牌 \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34; } # 发送请求 conn.request(\u0026#34;POST\u0026#34;, \u0026#34;/v1/chat/completions\u0026#34;, payload, headers) # 获取响应 res = conn.getresponse() data = res.read() # 输出结果 print(data.decode(\u0026#34;utf-8\u0026#34;)) 响应处理 # 当用户提问需要调用函数时，API会返回包含函数调用信息的JSON响应。开发者需要：\n解析响应中的函数调用请求\n在后端执行对应函数\n将函数结果再次发送给API获取最终回答\n注意事项 # 请妥善保管API密钥，不要泄露\n函数定义中的description很重要，会影响模型是否/如何调用该函数\n温度参数(temperature)控制生成结果的随机性，值越大结果越多样\nTools 场景示例 # 例如\n用户提问：“北京天气怎么样？”\n我们定义一个工具 get_weather(city: string)\nget_weather 函数需要用户开发者自己实现\n✅ 第一步：发送用户提问 + 工具定义 # curl --location --request POST \u0026#39;https://api.whatai.cc/v1/chat/completions\u0026#39; \\ --header \u0026#39;Authorization: Bearer $API_KEY\u0026#39; \\ --header \u0026#39;Content-Type: application/json\u0026#39; \\ --data-raw \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;gpt-4o\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;北京天气怎么样？\u0026#34; } ], \u0026#34;tools\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;function\u0026#34;, \u0026#34;function\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;get_weather\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;获取城市天气\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;city\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;城市名称，例如 北京\u0026#34; } }, \u0026#34;required\u0026#34;: [ \u0026#34;city\u0026#34; ] } } } ], \u0026#34;tool_choice\u0026#34;: \u0026#34;auto\u0026#34; }\u0026#39; 📥 AI 返回内容（模型决定调用 get_weather）：\n{ \u0026#34;choices\u0026#34;: [ { \u0026#34;message\u0026#34;: { \u0026#34;role\u0026#34;: \u0026#34;assistant\u0026#34;, \u0026#34;tool_calls\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;toolcall-abc123\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;function\u0026#34;, \u0026#34;function\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;get_weather\u0026#34;, \u0026#34;arguments\u0026#34;: \u0026#34;{ \\\u0026#34;city\\\u0026#34;: \\\u0026#34;北京\\\u0026#34; }\u0026#34; } } ] } } ] } 🔁 第二步：你执行实际函数（例如自己写的天气 API），并将结果通过工具响应发回 AI # 假设你查询天气后得到：\n{ \u0026quot;temperature\u0026quot;: \u0026quot;31°C\u0026quot;, \u0026quot;condition\u0026quot;: \u0026quot;晴\u0026quot; }\n那么你现在发送的是 完整上下文 + tool 响应：\ncurl --location --request POST \u0026#39;https://api.whatai.cc/v1/chat/completions\u0026#39; \\ --header \u0026#39;Authorization: Bearer $API_KEY\u0026#39; \\ --header \u0026#39;Content-Type: application/json\u0026#39; \\ --data-raw \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;gpt-4o\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;北京天气怎么样？\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;assistant\u0026#34;, \u0026#34;tool_calls\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;toolcall-abc123\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;function\u0026#34;, \u0026#34;function\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;get_weather\u0026#34;, \u0026#34;arguments\u0026#34;: \u0026#34;{ \\\u0026#34;city\\\u0026#34;: \\\u0026#34;北京\\\u0026#34; }\u0026#34; } } ] }, { \u0026#34;role\u0026#34;: \u0026#34;tool\u0026#34;, \u0026#34;tool_call_id\u0026#34;: \u0026#34;toolcall-abc123\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;{ \\\u0026#34;temperature\\\u0026#34;: \\\u0026#34;31°C\\\u0026#34;, \\\u0026#34;condition\\\u0026#34;: \\\u0026#34;晴\\\u0026#34; }\u0026#34; } ] }\u0026#39; ✅ 第三步：返回最终的自然语言回复（例如）： # { \u0026#34;choices\u0026#34;: [ { \u0026#34;message\u0026#34;: { \u0026#34;role\u0026#34;: \u0026#34;assistant\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;北京今天是晴天，气温为 31°C。适合出门哦！\u0026#34; } } ] } "},{"id":45,"href":"/docs/otherai/chat/SparkAI/","title":"SparkAI","section":"对话客户端类","content":" SparkAI # "},{"id":46,"href":"/docs/openai/whisper/","title":"whisper使用示例","section":"Openai请求格式（通用）","content":" whisper使用示例 # whisper 模型接口说明 # 该接口基于 Whisper 模型实现语音转文本功能，支持常见音频格式。\n基础概念 # Whisper模型: OpenAI 开源的语音识别模型，支持多语言转写\n音频格式: 支持 mp3、wav、m4a 等常见格式\n接口地址 # POST https://api.whatai.cc/v1/audio/transcriptions\n请求参数 # 参数名 类型 必填 说明 model string 是 固定值 \u0026ldquo;whisper-1\u0026rdquo; file file 是 要转写的音频文件 请求头 # Authorization: Bearer sk- **** **** **** **** **** * # 替换为你的 API 令牌\nPython 调用示例 # import json import requests def voice_to_text(file_path): \u0026#34;\u0026#34;\u0026#34; 语音转文本功能 参数: file_path: 音频文件路径 返回: 识别出的文本内容 \u0026#34;\u0026#34;\u0026#34; url = \u0026#34;https://api.whatai.cc/v1/audio/transcriptions\u0026#34; # 构造请求参数 payload = {\u0026#34;model\u0026#34;: \u0026#34;whisper-1\u0026#34;} files = {\u0026#34;file\u0026#34;: (\u0026#34;audio.mp3\u0026#34;, open(file_path, \u0026#34;rb\u0026#34;))} # 设置请求头(请替换为你的API密钥) headers = {\u0026#34;Authorization\u0026#34;: \u0026#34;Bearer sk-***************************\u0026#34;} # 替换为你的 API 令牌 # 发送POST请求 response = requests.post(url, headers=headers, data=payload, files=files) # 解析响应数据 data = json.loads(response.text) # 返回识别结果 return data.get(\u0026#34;text\u0026#34;, \u0026#34;\u0026#34;) # 使用示例 print(voice_to_text(\u0026#34;audio.mp3\u0026#34;)) # 替换为你的音频文件路径 响应示例 # 成功响应:\n{ \u0026#34;text\u0026#34;: \u0026#34;这是识别出的文本内容\u0026#34; } 注意事项 # 音频文件大小建议不超过25MB\n支持中文、英文等多种语言\n请妥善保管API密钥，不要泄露\n"},{"id":47,"href":"/docs/openai/gpttts/","title":"gpt-tts","section":"Openai请求格式（通用）","content":" gpt-tts # 接口说明 # 提供基于 gpt-4o-mini-tts TTS 模型的文本转语音服务，支持多种音色选择。\n基础概念 # TTS(Text-to-Speech): 将文本转换为自然语音的技术\n音色(Voice): 合成语音的声音特征，本API支持多种预设音色\n请求地址 # POST https://api.whatai.cc/v1/audio/speech\n请求头 # headers = { \u0026#34;Authorization\u0026#34;: \u0026#34;Bearer ******\u0026#34;, # 替换为您的API密钥 \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34; } 请求参数 # 参数名 类型 必填 说明 model string 是 固定值 \u0026ldquo;gpt-4o-mini-tts\u0026rdquo; input string 是 需要转换为语音的文本内容 voice string 是 音色类型，如 \u0026ldquo;alloy\u0026rdquo; Python 调用示例 # import requests import json url = \u0026#34;https://api.whatai.cc/v1/audio/speech\u0026#34; api_key = \u0026#34;******\u0026#34; # 替换为您的API密钥 payload = { \u0026#34;model\u0026#34;: \u0026#34;gpt-4o-mini-tts\u0026#34;, \u0026#34;input\u0026#34;: \u0026#34;我是API，欢迎使用语音合成服务\u0026#34;, \u0026#34;voice\u0026#34;: \u0026#34;alloy\u0026#34; } try: # 发送POST请求 response = requests.post(url, headers={\u0026#34;Authorization\u0026#34;: f\u0026#34;Bearer {api_key}\u0026#34;}, json=payload) # 检查响应状态 response.raise_for_status() # 处理音频响应 if response.headers[\u0026#34;Content-Type\u0026#34;] in (\u0026#34;audio/mpeg\u0026#34;, \u0026#34;audio/mp3\u0026#34;): with open(\u0026#34;output.mp3\u0026#34;, \u0026#34;wb\u0026#34;) as f: f.write(response.content) # 写入音频文件 print(\u0026#34;语音合成成功，已保存为output.mp3\u0026#34;) else: print(\u0026#34;错误响应:\u0026#34;, response.text) except Exception as e: print(f\u0026#34;请求出错: {e}\u0026#34;) 响应说明 # 成功: 返回MP3格式的音频流，Content-Type为audio/mpeg\n失败: 返回JSON格式的错误信息\n注意事项 # API密钥需妥善保管，不要暴露在客户端代码中\n输入文本长度建议不超过500字符\n音频采样率为24kHz，比特率128kbps\n"},{"id":48,"href":"/docs/otherai/chat/nineai/","title":"NineAI(99AI) ","section":"对话客户端类","content":" NineAI(99AI) # "},{"id":49,"href":"/docs/openai/gptwst/","title":"gpt文生图","section":"Openai请求格式（通用）","content":" gpt文生图 # 概念介绍 # 文生图(Text-to-Image)是一种通过自然语言描述生成对应图像的技术。本API基于OpenAI的GPT模型实现，支持多种图像生成模型和尺寸规格。\n基础信息 # 请求方式: POST\nBase URL: https://api.whatai.cc\n接口路径: /v1/images/generations\n认证方式: Bearer Token\n请求参数 # 参数名 类型 必填 说明 prompt string 是 图像描述文本 n int 否 生成图片数量(默认1) model string 否 模型选择(默认gpt-image-1) aspect_ratio string 否 宽高比(如\u0026quot;16:9\u0026quot;) size string 否 图像尺寸(如\u0026quot;1024x1536\u0026quot;) seed int 否 随机种子(-1表示随机) 支持的模型 # gpt-image-1: 基础模型(支持1024x1024,1024x1536,1536x1024)\nseedream-3.0: 国内最强，豆包团队开发，即梦3 AI绘图大模型。\ngpt-image-1: Openai GPT 的画图模型，文本理解与图像生成深度融合，适合文字驱动型创作\nimagen4: 谷歌的绘图模型，对标 gpt-image\nflux-kontext-max: Black Forest Labs推出商业级精度的图像生成，满足专业设计需求\nflux-kontext-pro: 支持文本+图像输入的上下文感知生成/编辑模型，控制更精准。\nPython调用示例 # import json import requests # API配置 API_KEY = \u0026#34;sk-********************\u0026#34; # 替换为你的API密钥 API_URL = \u0026#34;https://api.whatai.cc/v1/images/generations\u0026#34; # 请求参数 payload = { \u0026#34;prompt\u0026#34;: \u0026#34;哪吒竖着大拇指，背景广告牌写着 API\u0026#34;, # 图像描述 \u0026#34;n\u0026#34;: 1, # 生成数量 \u0026#34;model\u0026#34;: \u0026#34;gpt-image-1\u0026#34;, # 使用基础模型 \u0026#34;size\u0026#34;: \u0026#34;1024x1536\u0026#34;, # 图像尺寸 \u0026#34;seed\u0026#34;: -1 # -1 代表随机种子 } headers = { \u0026#34;Authorization\u0026#34;: f\u0026#34;Bearer {API_KEY}\u0026#34;, \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34; } try: # 发送POST请求 response = requests.post(API_URL, json=payload, headers=headers) response.raise_for_status() # 检查HTTP错误 # 解析响应数据 result = response.json() print(json.dumps(result, indent=4, ensure_ascii=False)) except requests.exceptions.RequestException as e: # 错误处理 print(f\u0026#34;请求失败: {e}\u0026#34;) if e.response: print(f\u0026#34;状态码: {e.response.status_code}\u0026#34;) print(f\u0026#34;响应内容: {e.response.text}\u0026#34;) 响应示例 # 图片返回base64需要转为png图片格式。\n成功响应将返回包含生成图像信息的JSON对象，其中可能包含：\ndata: 图像数据数组\nurl: 图像访问URL(部分模型)\nb64_json: Base64编码的图像数据\n{ \u0026#34;created\u0026#34;: 1677664795, \u0026#34;data\u0026#34;: [ { \u0026#34;url\u0026#34;: \u0026#34;https://.../generated-image.png\u0026#34;, \u0026#34;b64_json\u0026#34;: \u0026#34;...\u0026#34; } ] } 常见错误码： # 401: 认证失败(无效API密钥)\n400: 请求参数错误\n429: 请求频率限制\n500: 服务器内部错误\n建议在代码中添加完善的错误处理逻辑，如示例中所示。\n详细教程神马中转API # 🎨 神马中转API 2025超强画图模型天团驾到！ # \u0026ldquo;不会画画的API不是好艺术家\u0026rdquo; - 神马中转API用户如是说\n欢迎来到《手把手教你玩转AI画图》！这里有一群身怀绝技的\u0026quot;数字达芬奇\u0026quot;，随你调遣~\n🌟 模型天团简历（2025顶配版） # 模特名 后台大佬 必杀技 身价（每张） seedream-3.0 字节跳动豆包 文生图闪电侠⚡️ ￥0.08 (~免费10万张!) gpt-image-1 OpenAI 改图/合图魔术师🎩 ￥1+ imagen4 谷歌 GPT-image的宿敌👊 待定 flux-kontext Black Forest 商业级精修大师🖋️ ￥0.2~0.4 📌 温馨提示：\n不同模特在不同场景表现迥异，建议多约会试试~（比如seedream适合批量创作，gpt-image擅长精修）\n🚀 上车指南 # 1️⃣ 领钥匙：在 https://api.whatai.cc/token 获取API密钥\n2️⃣ 选车站：\nAPI_HOST = \u0026#34;api.whatai.cc\u0026#34; 💫 模特A：seedream-3.0（文生图小旋风） # import base64, http.client, json, os, time # 配置你的魔法棒🔮 API_KEY = \u0026#34;sk-******\u0026#34; # 此处填你的阿拉丁神灯密码 prompt = \u0026#34;漫画风格英语学习图：Hello单词记忆引导\u0026#34; # 你的脑洞有多大，舞台就有多大！ size = \u0026#34;1664x936\u0026#34; # 常用尺寸任选：16:9(宽屏) | 1:1(Ins风) | 9:16(手机壁纸) # 开始召唤！ conn = http.client.HTTPSConnection(\u0026#34;api.whatai.cc\u0026#34;) conn.request(\u0026#34;POST\u0026#34;, \u0026#34;/v1/images/generations\u0026#34;, json.dumps({ \u0026#34;prompt\u0026#34;: prompt, \u0026#34;model\u0026#34;: \u0026#34;seedream-3.0\u0026#34;, \u0026#34;size\u0026#34;: size, \u0026#34;response_format\u0026#34;: \u0026#34;b64_json\u0026#34; # 必选！否则收不到图片快递 }), headers={\u0026#34;Authorization\u0026#34;: f\u0026#34;Bearer {API_KEY}\u0026#34;}) # 拆快递啦~ res = conn.getresponse() if res.status == 200: img_data = json.loads(res.read())[\u0026#34;data\u0026#34;][0][\u0026#34;b64_json\u0026#34;] with open(f\u0026#34;seedream_{int(time.time())}.png\u0026#34;, \u0026#34;wb\u0026#34;) as f: f.write(base64.b64decode(img_data)) print(\u0026#34;🎉 你的大作已空投到文件夹！\u0026#34;) else: print(f\u0026#34;❌ 模特罢工了：{res.status} {res.reason}\u0026#34;) 💡 冷知识：当前免费额度能生成10万张图，够你把《蒙娜丽莎》画成表情包全集！\n🎩 模特B：gpt-image-1（PS战神） # import requests # 准备整容手术台 api_key = \u0026#34;sk-******\u0026#34; files = [ (\u0026#34;image\u0026#34;, (\u0026#34;原图.jpg\u0026#34;, open(\u0026#34;哪吒.jpg\u0026#34;, \u0026#34;rb\u0026#34;), \u0026#34;image/jpeg\u0026#34;)), # 要改造的对象 # (\u0026#34;image\u0026#34;, (\u0026#34;背景.jpg\u0026#34;, ...)) # 多图合并时加此项 ] payload = {\u0026#34;prompt\u0026#34;: \u0026#34;给哪吒戴上炫酷红色鸭舌帽\u0026#34;} # 你的改造指令 # 开始施法！ r = requests.post( \u0026#34;https://api.whatai.cc/v1/images/edits\u0026#34;, headers={\u0026#34;Authorization\u0026#34;: f\u0026#34;Bearer {api_key}\u0026#34;}, files=files, data=payload ) # 见证奇迹时刻✨ if r.status_code == 200: img_data = r.json()[\u0026#34;data\u0026#34;][0][\u0026#34;b64_json\u0026#34;] with open(\u0026#34;潮酷哪吒.png\u0026#34;, \u0026#34;wb\u0026#34;) as f: f.write(base64.b64decode(img_data)) print( 🌟 模特C：flux-kontext（商业级精修大师） # import requests import os import base64 from time import sleep # 准备你的魔法工具箱🛠️ API_KEY = \u0026#34;sk-******\u0026#34; # 你的专属魔法通行证 target_img = \u0026#34;沙漠.jpg\u0026#34; # 需要点石成金的照片 print(\u0026#34;🔮 正在召唤商业级修图精灵flux-kontext...\u0026#34;) sleep(1) # 加点魔法特效等待时间 try: # 开始施展专业级图像魔法 response = requests.post( \u0026#34;https://api.whatai.cc/v1/images/edits\u0026#34;, # 国际站小伙伴请用.com headers={\u0026#34;Authorization\u0026#34;: f\u0026#34;Bearer {API_KEY}\u0026#34;}, files=[(\u0026#34;image[]\u0026#34;, (os.path.basename(target_img), open(target_img, \u0026#34;rb\u0026#34;), \u0026#34;image/jpeg\u0026#34;))], data={ \u0026#34;model\u0026#34;: \u0026#34;flux-kontext-max\u0026#34;, # 可选：flux-kontext-pro（速度型）｜ flux-kontext-max（质量型） \u0026#34;prompt\u0026#34;: \u0026#34;给沙漠照片添加绿洲和彩虹\u0026#34;, # 你的想象力是唯一的限制 \u0026#34;quality\u0026#34;: \u0026#34;high\u0026#34; # 品质选项：low（快但糙）｜ medium｜ high（慢但精） } ) # 检查魔法是否生效 if response.status_code == 200: print(\u0026#34;🌈 魔法生效中...专业精灵正在精修您的照片\u0026#34;) img_data = response.json()[\u0026#34;data\u0026#34;][0][\u0026#34;b64_json\u0026#34;] # 保存你的魔法杰作 with open(\u0026#34;沙漠奇迹.jpg\u0026#34;, \u0026#34;wb\u0026#34;) as f: f.write(base64.b64decode(img_data)) print(\u0026#34;🎨 专业级大片已生成！建议立即设为壁纸炫耀~\u0026#34;) else: print(f\u0026#34;🧙‍♂️ 啊哦，魔法失灵了！错误代码：{response.status_code}\u0026#34;) print(f\u0026#34;🔍 详细原因：{response.text[:100]}...\u0026#34;) # 显示部分错误信息 except Exception as e: print(f\u0026#34;⚠️ 魔法反噬警告：{str(e)}\u0026#34;) print(\u0026#34;💡 小贴士：检查图片路径 | 网络连接 | API密钥有效性\u0026#34;) finally: print(\u0026#34;✨ 本次魔法仪式结束，期待您下次召唤！\u0026#34;) 专业小贴士：\nflux-kontext-pro和flux-kontext-max就像魔法学徒和魔法大师——前者快，后者强\n遇到复杂魔法（大图精修）时，喝杯咖啡耐心等待☕️\nquality参数是质量与速度的平衡术，按需调配\n❤️ 真爱提示 # 密钥保管：千万别把sk-***发到网上，否则你的AI模特会被别人拐跑！\n错误处理：遇到错误先检查——\n✔️ 域名写对了吗？\n✔️ 图片路径正确吗？\n✔️ 免费额度用完啦？\n玩得开心：多尝试不同咒语(prompt)，你会打开新世界大门！ 最后，神马中转API团队为你打Call：\u0026ldquo;让每个创意都轻松落地，是我们不变的初心~ ✨\u0026rdquo;\n🎉 毕业典礼 \u0026amp; 防坑指南 # # 所有模特通用的生存法则 print(\u0026#34;\u0026#34;\u0026#34; 🌟 三大黄金定律 🌟 1. 密钥保管好 -\u0026gt; 别让sk-***出现在GitHub上！ 2. 域名要选对 -\u0026gt; .cn还是.com？错配会变404幽灵👻 3. 尺寸看清楚 -\u0026gt; 别让16:9的美图硬塞进1:1相框！ \u0026#34;\u0026#34;\u0026#34;) print(\u0026#34;🎓 恭喜你完成AI画图大师课程！现在去征服世界吧~\u0026#34;) ✨ 教程到此结束，但你的创作之旅才刚刚开始！快去生成你的第一幅AI杰作吧~\n"},{"id":50,"href":"/docs/openai/gpttst/","title":"gpt图生图","section":"Openai请求格式（通用）","content":" gpt图生图 # 概念介绍 # 本API提供图像编辑功能，支持：\n单图修改：基于提示词对单张图片进行内容修改\n多图合并：将多张图片按提示词要求合并处理\n基础信息 # 请求方式：POST\nBase URL：https://api.whatai.cc/v1/images/edits\n认证方式：Bearer Token\n单图修改示例\nPython代码示例 # import base64 import json import requests # API配置 url = \u0026#34;https://api.whatai.cc/v1/images/edits\u0026#34; api_key = \u0026#34;sk-******\u0026#34; # 替换为你的API密钥 headers = { \u0026#34;Authorization\u0026#34;: f\u0026#34;Bearer {api_key}\u0026#34; } # 请求参数 payload = { \u0026#34;prompt\u0026#34;: \u0026#34;给哪吒带上一个红色的鸭舌帽，风格保持不变\u0026#34;, # 编辑指令 # \u0026#34;size\u0026#34;: \u0026#34;1024x1024\u0026#34; # 可选输出尺寸 } # 准备图片文件 files = [ (\u0026#34;image\u0026#34;, # 固定参数名 (\u0026#34;nezha.png\u0026#34;, # 文件名 open(\u0026#34;/path/to/nezha.png\u0026#34;, \u0026#34;rb\u0026#34;), # 文件路径 \u0026#34;image/png\u0026#34;) # 文件类型 ) ] # 发送请求 response = requests.post(url, headers=headers, data=payload, files=files) # 处理响应 if response.status_code == 200: try: data = response.json() # 提取base64编码的图片数据 if data.get(\u0026#34;data\u0026#34;) and isinstance(data[\u0026#34;data\u0026#34;], list): image_b64 = data[\u0026#34;data\u0026#34;][0].get(\u0026#34;b64_json\u0026#34;) if image_b64: # 解码并保存图片 with open(\u0026#34;output.png\u0026#34;, \u0026#34;wb\u0026#34;) as f: f.write(base64.b64decode(image_b64)) print(\u0026#34;图片保存成功\u0026#34;) else: print(\u0026#34;未获取到有效图片数据\u0026#34;) else: print(\u0026#34;响应数据结构异常\u0026#34;) except json.JSONDecodeError: print(\u0026#34;JSON解析失败\u0026#34;) else: print(f\u0026#34;请求失败: {response.status_code}\u0026#34;) 参数说明 # 参数名 必选 类型 说明 image 是 file 要编辑的图片文件 prompt 是 string 编辑指令描述 size 否 string 输出图片尺寸，如\u0026quot;1024x1024\u0026quot; 响应格式 # 成功响应示例：\n{ \u0026#34;data\u0026#34;: [ { \u0026#34;b64_json\u0026#34;: \u0026#34;base64编码的图片数据\u0026#34; } ] } 注意事项 # 图片文件需小于10MB\n支持PNG/JPEG格式\n编辑效果取决于提示词描述的准确性\n敏感操作需确保符合内容政策\n"}]